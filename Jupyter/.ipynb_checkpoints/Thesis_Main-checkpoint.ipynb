{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68e44fb",
   "metadata": {},
   "source": [
    "# Main Thesis Code \n",
    "\n",
    "This code contains all the functions and classes needed to generate results for determining error coefficients using least squares regression analysis of simulated data of an accelerometer going down the test track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2041c",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "Following cell imports all the libraries need to run the support functions and classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b661fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# from classes_x import *\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "import pandas as pd\n",
    "from sigfig import round\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'browser'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed5cb0",
   "metadata": {},
   "source": [
    "## Accelerometer Class\n",
    "### Description:\n",
    "The acceleromerter class defines an accelerometer object that contains an error model and a simulate function. The simulate function applies the associated error model attribute of the accelerombeter object and outputs what the output of the accelerometer would be given specific acceleration inputs. \n",
    "#### ___init___(self):\n",
    "The accelerometer class is initiatlized with an error model that determines the kind of error the accelerometer demonstrates. For the purpose of this effort only the scale-factor non-linearity terms were added as they are a major focus of recent sled testing efforts. These values are estimated values for strategic grade resonating beam accelerometers. The values below can be found in the table below. \n",
    "\n",
    "| Coefficient\t|Value\t      |Units\t       |Description                          |\n",
    "| :---          | :---:       | :---:          | :---                                |\n",
    "|$K_0$\t        |5\t          |$\\mu g$\t             |Bias                                 |\n",
    "|$K_1$\t\t    |.005         |$\\mu g/g $\t         |Scale Factor Error                   |\n",
    "|$K_2$\t        |60.144       |$\\mu \\frac{g}{g^2}$   |Scale factor 2nd order non-linearity |\n",
    "|$K_3$\t        |0.0152\t      |$\\mu \\frac{g}{g^3}$   |Scale factor 3rd order non-linearity |\n",
    "|$K_4$\t        |0.0058\t      |$\\mu \\frac{g}{g^4}$   |Scale factor 4th order non-linearity |\n",
    "|$K_5$\t        |0.0023       |$\\mu \\frac{g}{g^5}$   |Scale factor 5th order non-linearity |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881093c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accelerometer:\n",
    "    \n",
    "    def __init__(self):  # Default accelerometer characteristics\n",
    "     \n",
    "        self.g = 9.791807                     # Definition of g\n",
    "        \n",
    "        self.AccelModelCoef = {'K_1': 5            * 10**-6,      # Scale Factor (g/g) NEEDS UPDATED\n",
    "                               'K_0': .005         * 10**-6,      # Bias (g)\n",
    "                               'K_2': 61.14        * 10**-6,      # is second-order coefficient (g/g^2)\n",
    "                               'K_3': 0.02         * 10**-6,      # is third-order coefficient  (g/g^3)\n",
    "                               'K_4': 0.006        * 10**-6,      # is fourth-order coefficient (g/g^4)\n",
    "                               'K_5': 0.0023       * 10**-6       # is fifth-order coefficient  (g/g^5)\n",
    "                               }\n",
    "        \n",
    "        ## Other acceleromter error coefficients that could be added in the future.\n",
    "        # self.K_0_asym = 0                   # Bias Asymmetry \n",
    "        # self.K_1_asym = 0                   # Scale Factor Asymmetry\n",
    "        # self.K_oq = 0                       # Odd Quadratic Coefficient\n",
    "        # self.omeg_o = 0                    # is misalignmet of the IA with respect to the OA\n",
    "        # self.omeg_p = 0                    # is misalignmen of the IA with respect to the PA\n",
    "        # self.K_ip = 0                      # is crosscoupling coefficient \n",
    "        # self.K_io = 0                      # is crosscoupling coefficient\n",
    "        # self.K_po = 0                      # is crosscoupling coefficient\n",
    "        # self.K_pp = 1.32E-4 * 10**-6       # is cross-axis nonlinearity coefficients\n",
    "        # self.K_ppp = 2.10E-7 * 10**-6\n",
    "        # self.K_pppp = 2.3E-10 * 10**-6\n",
    "        # self.K_oo = 0                      # is cros-axis nonlinearity coefficients\n",
    "        # self.K_spin = 0                    # is spin correction coefficient, equal to \n",
    "        # self.K_ang_accel = 0               # is angular acceleration coefficient\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3378e",
   "metadata": {},
   "source": [
    "#### class *Accelerometer* \n",
    "#### function *simulate(self, a_i, n_start_idx, n_stop_idx)*\n",
    "\n",
    "##### Description: \n",
    "The simulate function simulates the output of a single acceleromter given accelation $(A_i)$ in g's along it's input axis. The accelerometer error $(A_{err})$ is given by the below equation\n",
    "\n",
    "$$A_{err}= K_0+K_1 A_{i}+K_2A_{i}^{2}+K_3A_{i}^{3}+K_4A_{i}^{4}+K_5A_{i}^{5}$$\n",
    "\n",
    "To get the actual output of the acceleromter the computed error is converted to ($m/s^2$) then added to the original input acceleration\n",
    "\n",
    "$$A_{sim} = g*(A_{err}) + A_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aaed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def simulate(self,a_i,n_start_idx, n_stop_idx):\n",
    "        \"\"\"\n",
    "        Starting with one dimensional error model. Outputs acceleration given\n",
    "        true input acceleration. In the future errors caused by inputs along the pendulus axis (a_p) \n",
    "        and output axis (a_o) \n",
    "        could be added.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert acceleration into g's since the error coefficients are defined in terms of g's. \n",
    "        g_i = a_i / self.g\n",
    "        \n",
    "        accel_model = [self.AccelModelCoef['K_1'] * (g_i),\n",
    "                       self.AccelModelCoef['K_0'] * np.ones(len(g_i)),  \n",
    "                       self.AccelModelCoef['K_2'] * (g_i**2), \n",
    "                       self.AccelModelCoef['K_3'] * (g_i**3), \n",
    "                       self.AccelModelCoef['K_4'] * (g_i**4), \n",
    "                       self.AccelModelCoef['K_5'] * (g_i**5)]\n",
    "        \n",
    "        # Add accelerometer error from each coefficient together and multiply by g then add original acceleration.\n",
    "        a_x_Sim = self.g * sum(accel_model[n_start_idx:n_stop_idx]) + a_i\n",
    "        \n",
    "        \n",
    "        return a_x_Sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16bfbf",
   "metadata": {},
   "source": [
    "## Generate Reference Trajectory\n",
    "### Description:\n",
    "These set of functions are used to generate a sled test trajectory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96a6d3",
   "metadata": {},
   "source": [
    "##### importEGIData(Headers,filepath)\n",
    "Imports data from a .csv file into columns titled using the inputed Headers list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76642fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importEGIData(Headers,filepath):\n",
    "        \n",
    "    if filepath == '':\n",
    "        print('No file selected')\n",
    "    else: \n",
    "        D = pd.read_csv(filepath , names = Headers) # Pull only first row from Excel File\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5b874",
   "metadata": {},
   "source": [
    "###### lpf(x, omege_c, T):\n",
    "This function filters inputted data using a first order low pass filter. This is used to smooth out accelerometer data collected from a real sled test for use as data to create the reference trajectory.\n",
    " - x = Input array.\n",
    " - omega_c = Cutoff frequency.\n",
    " - T = Sample time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpf(x, omega_c, T):\n",
    "    \"\"\"Implement a first-order low-pass filter.\n",
    "    \n",
    "    The input data is x, the filter's cutoff frequency is omega_c \n",
    "    [rad/s] and the sample time is T [s].  The output is y.\n",
    "    \"\"\"\n",
    "    N = np.size(x)\n",
    "    y = x\n",
    "    alpha = (2-T*omega_c)/(2+T*omega_c)\n",
    "    beta = T*omega_c/(2+T*omega_c)\n",
    "    for k in range(1, N):\n",
    "        y[k] = alpha*y[k-1] + beta*(x[k]+x[k-1])\n",
    "        \n",
    "    return y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963564dd",
   "metadata": {},
   "source": [
    "###### generateReferenceTrajectory()\n",
    "This function generates a reference trajectory given some inputted accleration data. For this implementation it takes in acceleration and velocity data from an EGI on board a sled test that took place at the HHSTT. \n",
    "\n",
    "generateReferenceTrajectory Steps:\n",
    "\n",
    "Step 1: Import Data.\n",
    "Real acceleration and velocity data was collected from an Embedded GPS/INS device mounted on a guidance sled test. All three axes of data is imported but only the values from the downtrack (X) axis is used in this implementation. \n",
    "\n",
    "Step 2: Cleaning data\n",
    "The data is then trimmed to focus on the part of the sled test where the actual launch occurs. The EGI sat for hours prior to launch during calibration of the unit under test but this data isn't necessary for this investigation. Occasionaly the time series from the raw data had repeated times for sucessive data points and so a new time series with an even sample rate was created and aplied to the data set to create a more even referenct trajectory. The new time series for the data was created by taking the total duration of the trajetory and dividing it by the total number of data points in the trajectory to get the new sample rate.\n",
    "\n",
    "Step 3: Smoothing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateReferenceTrajectory(plotcheck = False):\n",
    "    \n",
    "    # Define the file paths for acceleration and velocity data. \n",
    "    Accel_filepath = './EGI_data/EGI_accel.csv'\n",
    "    Vel_filepath = './EGI_data/EGI_accel.csv'\n",
    "    \n",
    "    # Save data into Pandas data frames with defined headers.\n",
    "    EGI_accel = importEGIData(['Time', 'Ax','Ay','Az'],Accel_filepath)\n",
    "    EGI_vel = importEGIData(['Time', 'Vx','Vy', 'Vz'],Vel_filepath)\n",
    "\n",
    "    # Combine the Acceleration and Velocity data frames into one.\n",
    "    EGI_accel_vel = EGI_accel.join(EGI_vel[['Vx','Vy','Vz']])\n",
    "\n",
    "    \n",
    "    \n",
    "    # Truth Gen Step 2 - Clean Data\n",
    "    \n",
    "    # Trim data to focus on actual sled run. These points were determined visually from the data used. \n",
    "    print('Developing Reference Trajectory')\n",
    "    print(\"Trimming data to start/stop time determined visually...\")\n",
    "    startTime = 399600   # Index of data of the beginning of the reference trajectory\n",
    "    stopTime = 399700    # Index of data of the end of the reference tracjectory\n",
    "\n",
    "    # Trim the reference trajectory to the start and stop indicies defined above.  \n",
    "    EGI_accel_vel_trim = EGI_accel_vel[(EGI_accel_vel['Time'] > startTime) & (EGI_accel_vel['Time'] < stopTime) ] # trim accelerometer output\n",
    "\n",
    "    # The data used for creating the reference trajectory had repeated time values for multiple measurements of velocitty and acceleration.\n",
    "    # To create a smooth reference trajectory the below code creates a new time series for the data by taking the total duration of the trajetory\n",
    "    # and dividing it by the total number of data points in the trajectory so the new sample rate is even across the whole trajetory.\n",
    "    \n",
    "    # Determine new time series parameters\n",
    "    Tdur = EGI_accel_vel_trim['Time'].max() - EGI_accel_vel_trim['Time'].min()\n",
    "    Tlen = len(EGI_accel_vel_trim['Time'])\n",
    "\n",
    "    # Generate new time series given duration of trajectory and number of data points.\n",
    "    NewTimeSeries = np.linspace(0, Tdur, Tlen)\n",
    "    \n",
    "    # Save new time series to Data Frame.\n",
    "    EGI_accel_vel_trim.loc[:,'New Time'] = NewTimeSeries\n",
    "                         \n",
    "        \n",
    "    #%% Truth Gen Step 3 - Smooth Acceleration in X-axis\n",
    "    \n",
    "    # Pull data from data frame\n",
    "    EGI_accel_presmoothed = EGI_accel_vel_trim[['Ax']]\n",
    "    \n",
    "    \n",
    "    EGI_accel_smoothed_array = lpf(EGI_accel_vel_trim[['Ax']].to_numpy(),50,Tdur/Tlen)\n",
    "\n",
    "    EGI_accel_vel_trim['Ax'] = EGI_accel_presmoothed\n",
    "\n",
    "    # EGI_accel_vel_trim['Ax_smooth'] = pd.Series(EGI_accel_smoothed_array)\n",
    "\n",
    "    #%% Truth Gen Step 4 - Create a DataFrame to house all truth data\n",
    "\n",
    "    referenceTrajectory = pd.DataFrame()\n",
    "\n",
    "    referenceTrajectory['Time'] = EGI_accel_vel_trim['New Time']\n",
    "    referenceTrajectory['refAccel_x'] = EGI_accel_smoothed_array\n",
    "    referenceTrajectory['refEGIVel_x'] = EGI_accel_vel_trim['Vx']\n",
    "\n",
    "    # Create New Time Series\n",
    "    referenceTrajectory['Time'] = np.linspace(0, Tdur, Tlen)\n",
    "\n",
    "    # Change initial acceleration in X to zero until launch. Determined visually\n",
    "    print(\"Setting initial acceleration to 0 until launch...\")\n",
    "    referenceTrajectory['refAccel_x'][:1145] = 0\n",
    "\n",
    "    # Change final acceleration after stop to zero. Determined visually\n",
    "    print(\"Setting final acceleration at 0...\")\n",
    "    referenceTrajectory['refAccel_x'][4968:] = 0\n",
    "    \n",
    "    \n",
    "    #%% Truth Gen Step 5 -  Integrate truth acceleration to get velocity and distance\n",
    "    referenceTrajectory['refVel_x'] = integrate.cumulative_trapezoid(y = referenceTrajectory['refAccel_x'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    \n",
    "    # Change final Velocity after stop to zero. Determined visually\n",
    "    print(\"Setting final velocity at 0...\")\n",
    "    referenceTrajectory['refVel_x'][4968:] = 0\n",
    "    \n",
    "    referenceTrajectory['refDist_x'] = integrate.cumulative_trapezoid(y = referenceTrajectory['refVel_x'],x = referenceTrajectory['Time'],initial = 0) \n",
    "\n",
    "\n",
    "    # Integrate EGI velocity to compare to double integrated acceleration\n",
    "    referenceTrajectory['refEGIDist_x'] = integrate.cumulative_trapezoid(y = referenceTrajectory['refEGIVel_x'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    \n",
    "    # Compute start motion time.\n",
    "    startMotionTime = referenceTrajectory['Time'][referenceTrajectory['refAccel_x']>0.001].iloc[0]\n",
    "    \n",
    "    referenceTrajectory['Time'] = referenceTrajectory['Time']-startMotionTime\n",
    "    \n",
    "    \n",
    "    #%% Save trajectory to Pickle File\n",
    "    \n",
    "    referenceTrajectory.to_pickle(\"./referenceTrajectory.pkl\")\n",
    "    \n",
    "    #%% Plots Acceleration and Velocity\n",
    "    if plotcheck == True:\n",
    "        Figure1 = PlotlyPlot()\n",
    "        \n",
    "        Figure1.setTitle('EGI Acceleration, Velocity and Smoothed acceleration')\n",
    "        Figure1.setYaxisTitle('Acceleration (m/s/s)')\n",
    "        Figure1.setYaxis2Title('Velocity (m/s)')\n",
    "        Figure1.setXaxisTitle('GPS Time (s)')\n",
    "        Figure1.settwoAxisChoice([False, True])\n",
    "        Figure1.plotTwoAxis(referenceTrajectory[['Ax','Vx']], df_x= EGI_accel_vel_trim[['New Time']])\n",
    "        Figure1.addLine(referenceTrajectory[['refAccel_x']], df_x = referenceTrajectory[['Time']],secondary_y=False)\n",
    "        Figure1.show()\n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "#%% Generate Track RPV Function \n",
    "\n",
    "def generateTrackRPV(referenceTrajectory, sigmaRPV, tauRPV, biasRPV, Overwrite=True):\n",
    "    \n",
    "    print(\"\\n Generating RPV\")\n",
    "    trackRPV = pd.DataFrame()\n",
    "    \n",
    "    # trackRPVzeroVel = \"NoZeroVel\"\n",
    "    trackRPVzeroVel = 'NoZeroVel'\n",
    "    \n",
    "    \n",
    "    if trackRPVzeroVel == \"NoZeroVel\":\n",
    "        print(\"No zero velocity portions of test selected\")\n",
    "    \n",
    "    Interupter_delta = 4.5 * 0.3048 # ft converted to meters\n",
    "    TrackLength = 10000   # Meters\n",
    "    \n",
    "    trackRPV['Interupters_DwnTrk_dist'] = np.arange(0, TrackLength, Interupter_delta)\n",
    "    \n",
    "    trackRPV['Time'] = np.interp(trackRPV['Interupters_DwnTrk_dist'],referenceTrajectory['refDist_x'],referenceTrajectory['Time'])\n",
    "    \n",
    "    trackRPV = trackRPV[trackRPV['Interupters_DwnTrk_dist'] <= referenceTrajectory['refDist_x'].max()]\n",
    "    \n",
    "    trackRPV = trackRPV.drop_duplicates(subset=['Time'])\n",
    "    \n",
    "    trackRPV = trackRPV[:-1]\n",
    "    \n",
    "    if trackRPVzeroVel != \"NoZeroVel\":\n",
    "        \n",
    "        trackRPV_zeroVel_start = pd.DataFrame() \n",
    "        trackRPV_zeroVel_start['Time'] = referenceTrajectory['Time'][referenceTrajectory['Time']<trackRPV['Time'].min()]\n",
    "        trackRPV_zeroVel_start['Interupters_DwnTrk_dist'] = 0\n",
    "        trackRPV_zeroVel_start = trackRPV_zeroVel_start.tail(2)\n",
    "    \n",
    "        trackRPV_zeroVel_end = pd.DataFrame()\n",
    "        \n",
    "        trackRPV_zeroVel_end['Time'] = referenceTrajectory['Time'][referenceTrajectory['refVel_x']==0]\n",
    "        trackRPV_zeroVel_end['Time'] = trackRPV_zeroVel_end['Time'][trackRPV_zeroVel_end['Time']>trackRPV['Time'].max()]\n",
    "        trackRPV_zeroVel_end['Interupters_DwnTrk_dist'] = referenceTrajectory['refDist_x'].max()\n",
    "        trackRPV_zeroVel_end = trackRPV_zeroVel_end.dropna()\n",
    "        \n",
    "        trackRPV_zeroVel_StartEnd = pd.DataFrame()\n",
    "        trackRPV_zeroVel_StartMidEnd = pd.DataFrame()\n",
    "        trackRPV_zeroVel_StartMid = pd.DataFrame()\n",
    "    \n",
    "        if trackRPVzeroVel == 'StartEnd':\n",
    "            trackRPV_zeroVel_StartEnd = pd.concat((trackRPV_zeroVel_start,trackRPV_zeroVel_end), axis = 0)\n",
    "            trackRPV_zeroVel_StartMidEnd = pd.concat((trackRPV, trackRPV_zeroVel_StartEnd), axis = 0)\n",
    "            trackRPV_zeroVel_StartMidEnd = trackRPV_zeroVel_StartMidEnd.sort_values(by='Time').reset_index(drop=True)\n",
    "            # trackRPV_zeroVel_StartMidEnd.to_pickle(\"./trackRPV_0Vel_StartEnd.pkl\")\n",
    "        elif trackRPVzeroVel == 'Start':\n",
    "            trackRPV_zeroVel_StartMid = pd.concat((trackRPV, trackRPV_zeroVel_start), axis = 0)\n",
    "            trackRPV_zeroVel_StartMid = trackRPV_zeroVel_StartMid.sort_values(by='Time').reset_index(drop=True)    \n",
    "            # trackRPV_zeroVel_StartMid.to_pickle(\"./trackRPV_0Vel_Start.pkl\")\n",
    "            trackRPV = trackRPV_zeroVel_StartMid\n",
    "    \n",
    "    \n",
    "    trackRPV = trackRPV.sort_values(by='Time').reset_index(drop=True)\n",
    "    \n",
    "    # trackRPV['Time'] = trackRPV['Time']-trackRPV['Time'][0]\n",
    "    \n",
    "    # Add error to Track RPV\n",
    "    if sigmaRPV != 0:\n",
    "        noise = np.random.normal(0,sigmaRPV,len(trackRPV)) # Add random noise to RPV\n",
    "        trackRPV['Interupters_DwnTrk_dist'] = trackRPV['Interupters_DwnTrk_dist'] + noise\n",
    "    \n",
    "    if tauRPV != 0:\n",
    "        trackRPV['Time'] = trackRPV['Time'] - tauRPV\n",
    "        \n",
    "    if biasRPV != 0:\n",
    "        trackRPV['Interupters_DwnTrk_dist'] = trackRPV['Interupters_DwnTrk_dist'] + biasRPV\n",
    "\n",
    "\n",
    "    #%% Save track RPV to pickle file\n",
    "    if Overwrite == True:\n",
    "        trackRPV.to_pickle(f\"./RPVs/trackRPV_sig{sigmaRPV}_tau{tauRPV}_bias{biasRPV}.pkl\")\n",
    "    else:\n",
    "       filepath = incrementFileName(f\"./VarianceRPVs/trackRPV_sig{sigmaRPV}_tau{tauRPV}_bias{biasRPV}.pkl\")\n",
    "       trackRPV.to_pickle(filepath)\n",
    "    return\n",
    "\n",
    "\n",
    "def AccelSim(referenceTrajectory, N_model, changeDefaultCoeff, CoeffDict, g):\n",
    "    \n",
    "    #%% ACCEL SIM Step 1 - Simulate a Acceleromter with Bias using Accelerometer class\n",
    "    \"\"\"\n",
    "    ACCEL SIM - Scripts used to generate simulated accelerometer output based on truth input\n",
    "    \n",
    "    Using smoothed acceleration truth data to simulate\n",
    "    \"\"\"\n",
    "    \n",
    "    AccelOne = Accelerometer()\n",
    "    \n",
    "    if changeDefaultCoeff == True:\n",
    "            AccelOne.AccelModelCoef.update(CoeffDict)\n",
    "    \n",
    "    AccelOne.g = g\n",
    "    \n",
    "    # Create data frame to house data\n",
    "    sensorSim = pd.DataFrame()\n",
    "    sensorSim['Time'] = referenceTrajectory['Time']\n",
    "    \n",
    "    # Change to array for us in simulation.\n",
    "    A_i_true = referenceTrajectory['refAccel_x'].to_numpy()  \n",
    "    \n",
    "    # Simulate\n",
    "    A_x_sim = AccelOne.simulate(A_i_true, N_model[0], N_model[1])  \n",
    "    \n",
    "    #Store data in data frame. \n",
    "    sensorSim['SensorSim_Ax'] = A_x_sim\n",
    "    \n",
    "    sensorSim['SensorSim_Ax'][referenceTrajectory['refAccel_x'] == 0] = 0\n",
    "    \n",
    "    #%% Integrate Simulated accelerations to develop Velocity and Displacement.\n",
    "    sensorSim['SensorSim_Vx'] = integrate.cumulative_trapezoid(y = sensorSim['SensorSim_Ax'],x = sensorSim['Time'],initial = 0) \n",
    "    sensorSim['SensorSim_Dx'] = integrate.cumulative_trapezoid(y = sensorSim['SensorSim_Vx'],x = sensorSim['Time'],initial = 0) \n",
    "    \n",
    "    AccelObj = AccelOne\n",
    "    \n",
    "    return [sensorSim, AccelObj]\n",
    "\n",
    "def RegressionAnalysis(referenceTrajectory, trackRPV, AccelObj, sensorSim, N_model, g,sigmaRPV, saveToPickel = False, WLSoption = True, LeastSquaresMethod = 'LongHand' ):\n",
    "    \n",
    "    #%% Error - Compare simulated acceleromter with track reference\n",
    "    \"\"\"\n",
    "    Error - Scripts used to compare accelerometer simulation versus track truth\n",
    "    \"\"\"\n",
    "    Dist_Error = pd.DataFrame()\n",
    "    Dist_Error['Time'] = trackRPV['Time']\n",
    "    \n",
    "    # Interpolate Sensor Sim to Track\n",
    "    \n",
    "    trackRPV['SensorDwnTrkDist'] = np.interp(trackRPV['Time'],sensorSim['Time'],sensorSim['SensorSim_Dx'])\n",
    "    \n",
    "    Dist_Error['DistErr_x'] = trackRPV['Interupters_DwnTrk_dist'] - trackRPV['SensorDwnTrkDist']\n",
    "    \n",
    "\n",
    "    # Compute Velocity Error\n",
    "    Ve_x = (np.diff(Dist_Error['DistErr_x'])/np.diff(Dist_Error['Time']))\n",
    "    Ve_t = (Dist_Error['Time'].head(-1) + np.diff(Dist_Error['Time'])/2).to_numpy()\n",
    "    \n",
    "    Error = pd.DataFrame()\n",
    "    \n",
    "    Error['Time'] = Ve_t\n",
    "    Error['SensorSim_Ax'] = np.interp(Ve_t,sensorSim['Time'],sensorSim['SensorSim_Ax']) \n",
    "    Error['SensorSim_Vx'] = np.interp(Ve_t,sensorSim['Time'],sensorSim['SensorSim_Vx'])\n",
    "    Error['SensorSim_Dx'] = np.interp(Ve_t,sensorSim['Time'],sensorSim['SensorSim_Dx'])\n",
    "    Error['DistErr_x'] = np.interp(Ve_t,Dist_Error['Time'],Dist_Error['DistErr_x']) \n",
    "    Error['VelErr_x'] = Ve_x\n",
    "     \n",
    "    #%% - Regression Analysis\n",
    "    \"\"\"\n",
    "    Regression Analysis - Scripts used to compute error model\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Compute coordinate functions\n",
    "    referenceTrajectory['Ax^2 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**2\n",
    "    referenceTrajectory['Ax^3 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**3\n",
    "    referenceTrajectory['Ax^4 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**4\n",
    "    referenceTrajectory['Ax^5 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**5\n",
    "    \n",
    "    referenceTrajectory['intAx^2 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^2 (g)'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    referenceTrajectory['intAx^3 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^3 (g)'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    referenceTrajectory['intAx^4 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^4 (g)'],x = referenceTrajectory['Time'],initial = 0)\n",
    "    referenceTrajectory['intAx^5 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^5 (g)'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    \n",
    "    \n",
    "    Vx = np.interp(Ve_t, referenceTrajectory['Time'],referenceTrajectory['refVel_x'])\n",
    "    intAx_2 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^2 (g)']) \n",
    "    intAx_3 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^3 (g)']) \n",
    "    intAx_4 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^4 (g)']) \n",
    "    intAx_5 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^5 (g)'])\n",
    "    \n",
    "    coordinateFunctionDF = pd.DataFrame()\n",
    "    coordinateFunctionDF['Time'] = Ve_t\n",
    "     \n",
    "    \n",
    "    coeff_dict = {'Est_V_0': 0, 'Est_K_1': 0, 'Est_K_0': 0, 'Est_K_2': 0, 'Est_K_3': 0, 'Est_K_4': 0, 'Est_K_5': 0}\n",
    "    \n",
    "    # Create Complete A Matrix\n",
    "    complete_A = np.array([np.ones(len(Ve_t))/g, -Vx/g, -Ve_t, intAx_2, intAx_3, intAx_4, intAx_5])*g\n",
    "    complete_A = complete_A.T\n",
    "    \n",
    "    complete_A_DF = pd.DataFrame(np.fliplr(complete_A), columns=['IntAx_5', 'IntAx_4', 'IntAx_3', 'IntAx_2', 'Ve_t', 'Vx', 'Ones'])\n",
    "    \n",
    "    trimmed_A_filt = np.zeros(complete_A.shape[1], dtype = bool)\n",
    "    trimmed_A_filt[0] = 1\n",
    "    \n",
    "    trimmed_A_filt[N_model[0]+1:N_model[1]+1] = 1\n",
    "\n",
    "    trimmed_A = complete_A[:,trimmed_A_filt]\n",
    "    \n",
    "    '''\n",
    "    COMPUTE COVARIANCE\n",
    "    '''\n",
    "\n",
    "    #%% Compute Covariance    \n",
    "    \n",
    "    #%% \n",
    "    # Linear Regression\n",
    "    coeff_list = tuple(None for _ in range(trimmed_A.shape[1]))\n",
    "\n",
    "    \n",
    "    if sigmaRPV == 0 or WLSoption == False: \n",
    "        size = trimmed_A.shape[0]\n",
    "        W = np.identity(size)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Develop weighted matrix\n",
    "        delta_t = np.diff(trackRPV['Time'])\n",
    "        vel_sig = np.sqrt(2)*sigmaRPV/delta_t\n",
    "        \n",
    "        W = np.diag(vel_sig,0) - np.diag((.5*vel_sig[1:]),-1) - np.diag((.5*vel_sig[1:]),1)\n",
    "    \n",
    "        W = np.linalg.inv(W)\n",
    "        \n",
    "        # W = np.diag(1/vel_sig) \n",
    "        \n",
    "    A = trimmed_A\n",
    "    \n",
    "    AW = np.transpose(trimmed_A).dot(W)\n",
    "    Ve_xW = W.dot(Ve_x)\n",
    "    \n",
    "\n",
    "   \n",
    "    if LeastSquaresMethod == 'Numpy':\n",
    "\n",
    "        coeff_list = np.linalg.lstsq(np.transpose(AW), Ve_xW, rcond=None)[0] # This has just been used for debugging to check if \"Long\" least squares leads to same results.\n",
    "    \n",
    "    elif LeastSquaresMethod == 'SciKit':\n",
    "        testSKlearn = LinearRegression()\n",
    "        testSKlearn.fit(trimmed_A, Ve_x, sample_weight=(np.diag(W)))\n",
    "        coeff_list = testSKlearn.coef_\n",
    "        coeff_list[0] = testSKlearn.intercept_\n",
    "    \n",
    "    elif LeastSquaresMethod == 'LongHand':\n",
    "        At = np.transpose(trimmed_A)\n",
    "        coeff_list = np.linalg.inv(At.dot(W).dot(trimmed_A)).dot(At).dot(W).dot(Ve_x)\n",
    "        \n",
    "    else: \n",
    "        print(\"Did not select an applicable Least Squares Method\")\n",
    "\n",
    "\n",
    "    covariance_A = np.linalg.inv(np.dot(AW,trimmed_A))\n",
    "\n",
    "    print_List = np.array(list(coeff_dict.keys()))\n",
    "    \n",
    "    n = 0\n",
    "    for coef in print_List[trimmed_A_filt]:\n",
    "        coeff_dict[coef] = coeff_list[n]\n",
    "        n += 1\n",
    "    \n",
    "    #%% Save results to DataFrame\n",
    "    \n",
    "    coefficientDF = pd.DataFrame()\n",
    "    \n",
    "    coefficientDF = pd.concat((coefficientDF, pd.DataFrame.from_dict(AccelObj.AccelModelCoef, orient = 'index', columns= ['Accel Model'])))\n",
    "    \n",
    "    coefficientDF.loc['V_0'] = 0\n",
    "    \n",
    "    # Build Estimated Coefficient DF\n",
    "    estimatedCoefficients = pd.DataFrame.from_dict(coeff_dict, orient = 'index', columns= ['Estimated Coefficients'])\n",
    "    \n",
    "    renameDict = {}\n",
    "    for coeff in print_List:\n",
    "        renameDict[coeff] = coeff[4:]\n",
    "        \n",
    "    estimatedCoefficients = estimatedCoefficients.rename(index = renameDict) \n",
    "    estimatedCoefficients.replace(0, np.nan, inplace=True)\n",
    "    \n",
    "    \n",
    "    coefficientDF = pd.merge(coefficientDF,estimatedCoefficients,left_index=True, right_index=True)\n",
    "    \n",
    "    coefficientDF['Coefficient Estimate Error'] = coefficientDF['Accel Model'] - coefficientDF['Estimated Coefficients']\n",
    "    \n",
    "                \n",
    "    #%% Compute Velocity Error Residuals\n",
    "    \n",
    "    V_error_model_terms = [coeff_dict['Est_V_0'], \n",
    "                           coeff_dict['Est_K_1']*Vx,  \n",
    "                           coeff_dict['Est_K_0']*Ve_t, \n",
    "                           coeff_dict['Est_K_2']*intAx_2, \n",
    "                           coeff_dict['Est_K_3']*intAx_3,  \n",
    "                           coeff_dict['Est_K_4']*intAx_4,  \n",
    "                           coeff_dict['Est_K_5']*intAx_5]\n",
    "    \n",
    "    Error['V_error_model'] = sum(V_error_model_terms)*g \n",
    "    Error['Ve_x_Resid'] = Error['VelErr_x'] - Error['V_error_model'] \n",
    "    \n",
    "  \n",
    "    #%% Save off results:\n",
    "    if saveToPickel == True:\n",
    "        Error.to_pickle(f\"./ErrorDF_{N_model[0]}-{N_model[1]}.pkl\")\n",
    "        coefficientDF.to_pickle(f\"./coefficientDF_{N_model[0]}-{N_model[1]}.pkl\")\n",
    "        \n",
    "\n",
    "    return [coefficientDF, Error, covariance_A, A, Ve_x, W, LeastSquaresMethod]\n",
    "\n",
    "def figText(text):\n",
    "\n",
    "    LaTeXText = '$\\\\text{' + text + ' }$'\n",
    "\n",
    "    return LaTeXText\n",
    "\n",
    "def round_array_to_sigfigs(array, sigfigs):\n",
    "    rounded_array = np.zeros_like(array)  # Create an array of zeros with the same shape as the input array\n",
    "    \n",
    "    for i in range(array.shape[0]):\n",
    "        for j in range(array.shape[1]):\n",
    "            if array[i, j] == 0:\n",
    "                rounded_array[i, j] = 0\n",
    "            else:\n",
    "                rounded_array[i, j] = round(array[i, j], sigfigs-1-int(np.floor(np.log10(np.abs(array[i, j])))))  # Calculate the number of decimals based on significant figures\n",
    "    \n",
    "    return rounded_array\n",
    "\n",
    "\n",
    "def incrementFileName(base_path):\n",
    "    \n",
    "    # initialize the increment variable\n",
    "    increment = 0\n",
    "    \n",
    "    # loop until we find a file name that doesn't exist\n",
    "\n",
    "    while True:\n",
    "        # create the file path with the increment\n",
    "        file_path = f\"{os.path.splitext(base_path)[0]}_{increment}{os.path.splitext(base_path)[1]}\"\n",
    "    \n",
    "        # check if the file exists\n",
    "        if os.path.isfile(file_path):\n",
    "            # if it does, increment the counter and try again\n",
    "            increment += 1\n",
    "        else:\n",
    "            # if it doesn't, break out of the loop\n",
    "            break\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e357481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50ff7c8c",
   "metadata": {},
   "source": [
    "## PlotlyPlot class\n",
    "#### Description:\n",
    "\n",
    "This plotly class is just a helper class that makes ploting Plotly plots easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc679dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSimple(df, x = None, y = None):\n",
    "    \n",
    "    if x == None and y == None:\n",
    "        fig = px.line(df)\n",
    "        fig.show()\n",
    "    elif y == None:\n",
    "        fig = px.line(df, x = x)\n",
    "        fig.show()\n",
    "    else:\n",
    "        fig = px.line(df, x = x, y = y)\n",
    "        fig.show()    \n",
    "    \n",
    "    return\n",
    "\n",
    "class PlotlyPlot:\n",
    "        \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.title = ''\n",
    "        self.x_axis = ''\n",
    "        self.y_axis = ''\n",
    "        self.y_axis_2 = ''\n",
    "        self.twoAxisChoice = [False,True]\n",
    "        self.template = 'simple_white'\n",
    "        \n",
    "    def setXaxisTitle(self,title):\n",
    "        # if title[0] != '$':\n",
    "        #     self.x_axis = self.figText(title)\n",
    "        # else:        \n",
    "        #     self.x_axis = title\n",
    "        # return\n",
    "    \n",
    "        self.x_axis = title\n",
    "    \n",
    "    def setYaxisTitle(self,title):\n",
    "        # if title[0] != '$':\n",
    "        #     self.y_axis = self.figText(title)\n",
    "        # else:        \n",
    "        #     self.y_axis = title\n",
    "        # return\n",
    "    \n",
    "        self.y_axis = title\n",
    "    \n",
    "    def setYaxis2Title(self,title):\n",
    "        # if title[0] != '$':\n",
    "        #     self.y_axis_2 = self.figText(title)\n",
    "        # else:        \n",
    "        #     self.y_axis_2 = title\n",
    "        # return\n",
    "    \n",
    "        self.y_axis_2 = title\n",
    "    \n",
    "    \n",
    "    def setTitle(self,title):\n",
    "        # if title[0] != '$':\n",
    "        #     self.title = self.figText(title)\n",
    "        # else:        \n",
    "        #     self.title = title\n",
    "        # return\n",
    "    \n",
    "        self.title = title\n",
    "    \n",
    "    def settwoAxisChoice(self,twoAxisChoice):\n",
    "        self.twoAxisChoice = twoAxisChoice\n",
    "        return\n",
    "    \n",
    "    def plotNoDF(self, X = np.empty((0,)), Y = np.empty((0,)), Mode = 'lines', Name = None, Opacity = 1, Size = None):\n",
    "        \n",
    "        if X.size == 0:\n",
    "            self.fig = go.Figure(go.Scatter(y = Y, name = Name, mode = Mode,  opacity = Opacity))\n",
    "        else:\n",
    "            self.fig = go.Figure(go.Scatter(x = X, y = Y, name = Name, mode = Mode,  opacity = Opacity))\n",
    "\n",
    "        # Add Title \n",
    "        self.fig.update_layout(\n",
    "            title_text = self.title)\n",
    "        \n",
    "        # Add Axis Labels\n",
    "        self.fig.update_xaxes(title_text = self.x_axis)\n",
    "        self.fig.update_yaxes(title_text = self.y_axis)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def addScatterNoDF(self,X = np.empty((0,)), Y = np.empty((0,)), Mode = 'lines', Name = None, Opacity = 1, Size = None, secondary_y = None):\n",
    "        \n",
    "        if secondary_y != None:           \n",
    "            self.twoAxisChoice.append(secondary_y)\n",
    "        \n",
    "        if X.size == 0:\n",
    "            self.fig.add_trace(go.Scatter(y = Y, name = Name, mode = Mode,  opacity = Opacity), secondary_y=secondary_y)\n",
    "        else:\n",
    "            self.fig.add_trace(go.Scatter(x = X, y = Y, name = Name, mode = Mode,  opacity = Opacity), secondary_y=secondary_y)\n",
    "\n",
    "\n",
    "    def plotSimple(self,df, x = None, y = None):\n",
    "        \n",
    "        if x == None and y == None:\n",
    "            self.fig = px.line(df)\n",
    "            self.fig.show()\n",
    "        elif y == None:\n",
    "            self.fig = px.line(df, x = x)\n",
    "            self.fig.show()\n",
    "        else:\n",
    "            self.fig = px.line(df, x = x, y = y)\n",
    "            self.fig.show()    \n",
    "            \n",
    "        return\n",
    "\n",
    "    def plotTwoAxis(self, df, df_x, Mode = 'lines', Name = None, Opacity = 1, Size = None):\n",
    "        \n",
    "        #df is a dataframe\n",
    "        #LeftRight is a list of booleans that determine which y data gets plotted on second axis\n",
    "        \n",
    "        self.fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "        \n",
    "        count = 0     \n",
    "       \n",
    "        for col in df:\n",
    "            # Add Traces\n",
    "            if Name != None: \n",
    "                self.fig.add_trace(\n",
    "                    go.Scatter(x = df_x.iloc[:,0], y = df[col], name = Name, mode = Mode,  opacity = Opacity),\n",
    "                    secondary_y = self.twoAxisChoice[count],)\n",
    "            else:\n",
    "                self.fig.add_trace(\n",
    "                    go.Scatter(x = df_x.iloc[:,0], y = df[col], name = col, mode = Mode,  opacity = Opacity),\n",
    "                    secondary_y = self.twoAxisChoice[count],)\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "        #Change size of markers\n",
    "        if Size != None:\n",
    "            self.update_marker_size(Size)\n",
    "            \n",
    "        # Add Title \n",
    "        self.fig.update_layout(\n",
    "            title_text = self.title)\n",
    "        \n",
    "        # Add Axis Labels\n",
    "        self.fig.update_xaxes(title_text = self.x_axis)\n",
    "        self.fig.update_yaxes(title_text = self.y_axis, secondary_y = False)\n",
    "        self.fig.update_yaxes(title_text = self.y_axis_2, secondary_y = True)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def addScatter(self,df, df_x, secondary_y = None, Name = None, Mode = 'markers', Opacity = 1, Size = None):\n",
    "        \n",
    "        if Name == None:\n",
    "            Name = df.columns.values[0]\n",
    "            \n",
    "        if secondary_y != None:\n",
    "            self.twoAxisChoice.append(secondary_y)\n",
    "            self.fig.add_trace(go.Scatter(x = df_x.iloc[:,0],y = df.iloc[:,0], name = Name, mode = Mode, opacity = Opacity),secondary_y = secondary_y)\n",
    "        else:\n",
    "            self.fig.add_trace(go.Scatter(x = df_x.iloc[:,0],y = df.iloc[:,0], name = Name, mode = Mode, opacity=Opacity))\n",
    "            \n",
    "        #Change size of markers\n",
    "        if Size != None:\n",
    "            self.update_marker_size(Size)\n",
    "    \n",
    "    def addLine(self,df, df_x, secondary_y = None, Name = None, Opacity = 1):\n",
    "        \n",
    "        name = df.columns.values[0]\n",
    "        \n",
    "        if secondary_y != None:\n",
    "            self.twoAxisChoice.append(secondary_y)\n",
    "            self.fig.add_trace(go.Scatter(x = df_x.iloc[:,0],y = df.iloc[:,0], name = Name, opacity = Opacity),secondary_y = secondary_y)\n",
    "        else:\n",
    "            self.fig.add_trace(go.Scatter(x = df_x.iloc[:,0],y = df.iloc[:,0], name = Name, opacity = Opacity))\n",
    "    \n",
    "    def legendTopLeft(self):\n",
    "        self.fig.update_layout(legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "            ))\n",
    "        \n",
    "    def legendTopRight(self):\n",
    "        self.fig.update_layout(\n",
    "            legend=dict(\n",
    "                x=.9,\n",
    "                y=1,\n",
    "                xanchor='right',\n",
    "                yanchor='top'\n",
    "            ))\n",
    "        \n",
    "    def update_template(self, Template = 'simple_white'):\n",
    "        self.fig.update_layout(template = Template)\n",
    "        self.fig.update_layout(font=dict(family=\"Serif\"))\n",
    "        \n",
    "    def update_legend(self):\n",
    "        self.fig.update_layout(legend=dict(font=dict(family = 'Arial')))\n",
    "\n",
    "    \n",
    "    def write_image(self, figName, path):\n",
    "        scale_factor = 1.2\n",
    "        self.fig.write_image(f\"{path}/{figName}.pdf\", width = 600*scale_factor, height = 400*scale_factor)  \n",
    "        self.fig.write_image(f\"{path}/{figName}.png\", width = 600*scale_factor, height = 400*scale_factor, scale=5)\n",
    "        \n",
    "    def addZoomSubPlot(self, zoom_x, zoom_y, Opacity=1):\n",
    "        # zoom_x and zoom_y are the x and y coordinates of the new zoom window.\n",
    "        # zoom_x = [x1, x2]\n",
    "        # zoom_y = [y1, y2]\n",
    "        \n",
    "        #Initialize Subplot \n",
    "        traces = self.fig.data #Take all traces from first figure\n",
    "        \n",
    "        self.fig = make_subplots(rows=1, cols=2)\n",
    "        \n",
    "        colorsG10 = ['#3366CC', '#DC3912', '#FF9900', '#109618', '#990099', '#0099C6', '#DD4477', '#66AA00', '#B82E2E', '#316395']\n",
    "        color_i = 0\n",
    "        # Add all traces back into both figures\n",
    "        \n",
    "        for trace in traces:\n",
    "            if trace.line != None:\n",
    "                trace.update(opacity=Opacity, line=dict(color = colorsG10[color_i]))\n",
    "                self.fig.add_trace(trace, row=1, col=1)\n",
    "                self.fig.add_trace(trace, row=1, col=2)\n",
    "                self.fig.data[-1].showlegend = False\n",
    "                \n",
    "            if color_i <= len(colorsG10):   \n",
    "                color_i += 1\n",
    "            else: \n",
    "                color_i = 0\n",
    "                \n",
    "        \n",
    "        # Update zoom of subplot\n",
    "        self.fig.update_xaxes(range=zoom_x, row=1, col=2)\n",
    "        self.fig.update_yaxes(range=zoom_y, row=1, col=2)\n",
    "        \n",
    "        # Add box around zoom area\n",
    "        self.addShadedBox(zoom_x, zoom_y, Row = 1, Col = 1)\n",
    "\n",
    "   \n",
    "    def addBox(self, box_x, box_y, Row=1, Col=1, scale_factor_x=1, scale_factor_y=1):\n",
    "        \n",
    "        box_X_scaled, box_Y_scaled = self.scale_rectangle(box_x, box_y, scale_factor_x,scale_factor_y)\n",
    "        \n",
    "        # Create the lines connecting the corners of the box to the corners of the second figure\n",
    "        box_trace = go.Scatter(\n",
    "            x=[box_X_scaled[0], box_X_scaled[0], box_X_scaled[1], box_X_scaled[1],box_X_scaled[0]],  # X coordinates for the lines\n",
    "            y=[box_Y_scaled[0], box_Y_scaled[1], box_Y_scaled[1], box_Y_scaled[0],box_Y_scaled[0]], # Y coordinates for the lines\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=1),\n",
    "            name = 'Zoomed Area' # Customize the line color, width, and style\n",
    "        )\n",
    "        self.fig.add_trace(box_trace, row = Row, col = Col) # Add box trace to original figure.\n",
    "        \n",
    "    def addShadedBox(self, box_x, box_y, Row=1, Col=1, scale_factor_x=1, scale_factor_y=1):\n",
    "    \n",
    "        box_X_scaled, box_Y_scaled = self.scale_rectangle(box_x, box_y, scale_factor_x,scale_factor_y)\n",
    "        \n",
    "        shape = go.layout.Shape(\n",
    "            type=\"rect\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            x0=box_X_scaled[0],\n",
    "            y0=box_Y_scaled[0],\n",
    "            x1=box_X_scaled[1],\n",
    "            y1=box_Y_scaled[1],\n",
    "            fillcolor=\"lightblue\",\n",
    "            opacity=0.3,\n",
    "            line=dict(color='black', width=1)\n",
    "            \n",
    "        )    \n",
    "        \n",
    "        self.fig.add_shape(shape,layer='below')\n",
    "        \n",
    "        self.addBox(box_x,box_y, Row=Row, Col=Col, scale_factor_x=scale_factor_x, scale_factor_y=scale_factor_y)\n",
    "     \n",
    "    def addLineShape(self, line_x, line_y, Row=1, Col=1):\n",
    "        \n",
    "        \n",
    "        self.fig.add_shape(type=\"line\",\n",
    "                      x0=line_x[0], y0=line_y[0], x1=line_x[1], y1=line_y[1],\n",
    "                      row=1, col=2,\n",
    "                      line=dict(color=\"red\", width=2))    \n",
    "    \n",
    "    def zoom(self, zoom_x, zoom_y, Row = 1, Col = 1):\n",
    "        self.fig.update_xaxes(range=zoom_x, row=Row, col=Col)\n",
    "        self.fig.update_yaxes(range=zoom_y, row=Row, col=Col)\n",
    "    \n",
    "    \n",
    "    def show(self):\n",
    "        self.fig.show()\n",
    "        return\n",
    "    \n",
    "    def figText(self, text):\n",
    "\n",
    "        LaTeXText = '$\\\\text{' + text + ' }$'\n",
    "\n",
    "        return LaTeXText\n",
    "    \n",
    "    def update_marker_size(self, Size):\n",
    "        \n",
    "        #Get traces from Figure\n",
    "        traces = self.fig.data\n",
    "        import plotly.graph_objects as go\n",
    "\n",
    "    def update_marker_size(self, marker_size):\n",
    "  \n",
    "        for data in self.fig.data:\n",
    "            if 'marker' in data:\n",
    "                data.marker.size = marker_size\n",
    "                \n",
    "        # self.fig.update_layout(legend= {'itemsizing': 'constant'})\n",
    "       \n",
    "    \n",
    "    def scale_rectangle(self, x_coords, y_coords, scale_factor_x, scale_factor_y):\n",
    "        x1 = x_coords[0]\n",
    "        x2 = x_coords[1]\n",
    "        y1 = y_coords[0]\n",
    "        y2 = y_coords[1]\n",
    "        \n",
    "        # Calculating the center of the rectangle\n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "    \n",
    "        # Calculating the width and height of the rectangle\n",
    "        width = abs(x2 - x1)\n",
    "        height = abs(y2 - y1)\n",
    "    \n",
    "        # Scaling up the rectangle\n",
    "        new_width = width * scale_factor_x\n",
    "        new_height = height * scale_factor_y\n",
    "    \n",
    "        # Calculating the new coordinates of the rectangle\n",
    "        new_x1 = center_x - new_width / 2\n",
    "        new_y1 = center_y - new_height / 2\n",
    "        new_x2 = center_x + new_width / 2\n",
    "        new_y2 = center_y + new_height / 2\n",
    "        \n",
    "        new_x_coords = [new_x1, new_x2]\n",
    "        new_y_coords = [new_y1, new_y2]\n",
    "    \n",
    "        return new_x_coords, new_y_coords   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
