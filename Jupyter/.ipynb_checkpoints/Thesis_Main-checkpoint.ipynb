{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68e44fb",
   "metadata": {},
   "source": [
    "# Main Thesis Code \n",
    "\n",
    "This code contains all the functions and classes needed to generate results for determining error coefficients using least squares regression analysis of simulated data of an accelerometer going down the test track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2041c",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "Following cell imports all the libraries need to run the support functions and classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b661fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# from classes_x import *\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "import pandas as pd\n",
    "from sigfig import round\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'browser'\n",
    "\n",
    "import pdb\n",
    "\n",
    "%run Thesis_Utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed5cb0",
   "metadata": {},
   "source": [
    "<a id='Accelerometer_Class'></a>\n",
    "\n",
    "## Accelerometer Class\n",
    "\n",
    "### Description:\n",
    "The acceleromerter class defines an accelerometer object that contains an error model and a simulate function. The simulate function applies the associated error model attribute of the accelerombeter object and outputs what the output of the accelerometer would be given specific acceleration inputs. \n",
    "#### ___init___(self):\n",
    "The accelerometer class is initiatlized with an error model that determines the kind of error the accelerometer demonstrates. For the purpose of this effort only the scale-factor non-linearity terms were added as they are a major focus of recent sled testing efforts. These values are estimated values for strategic grade resonating beam accelerometers. The values below can be found in the table below. \n",
    "\n",
    "| Coefficient\t|Value\t      |Units\t       |Description                          |\n",
    "| :---          | :---:       | :---:          | :---                                |\n",
    "|$K_0$\t        |5\t          |$\\mu g$\t             |Bias                                 |\n",
    "|$K_1$\t\t    |.005         |$\\mu g/g $\t         |Scale Factor Error                   |\n",
    "|$K_2$\t        |60.144       |$\\mu \\frac{g}{g^2}$   |Scale factor 2nd order non-linearity |\n",
    "|$K_3$\t        |0.0152\t      |$\\mu \\frac{g}{g^3}$   |Scale factor 3rd order non-linearity |\n",
    "|$K_4$\t        |0.0058\t      |$\\mu \\frac{g}{g^4}$   |Scale factor 4th order non-linearity |\n",
    "|$K_5$\t        |0.0023       |$\\mu \\frac{g}{g^5}$   |Scale factor 5th order non-linearity |\n",
    "\n",
    "\n",
    "#### function *simulate(self, a_i, n_start_idx, n_stop_idx)*\n",
    "\n",
    "##### Description: \n",
    "The simulate function simulates the output of a single acceleromter given accelation $(A_i)$ in g's along it's input axis. The accelerometer error $(A_{err})$ is given by the below equation\n",
    "\n",
    "$$A_{err}= K_0+K_1 A_{i}+K_2A_{i}^{2}+K_3A_{i}^{3}+K_4A_{i}^{4}+K_5A_{i}^{5}$$\n",
    "\n",
    "To get the actual output of the acceleromter the computed error is converted to ($m/s^2$) then added to the original input acceleration\n",
    "\n",
    "$$A_{sim} = g*(A_{err}) + A_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881093c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accelerometer:\n",
    "    \n",
    "    def __init__(self):  # Default accelerometer characteristics\n",
    "     \n",
    "        self.g = 9.791807                     # Definition of g\n",
    "        \n",
    "        self.AccelModelCoef = {'K_1': 5            * 10**-6,      # Scale Factor (g/g) NEEDS UPDATED\n",
    "                               'K_0': .005         * 10**-6,      # Bias (g)\n",
    "                               'K_2': 61.14        * 10**-6,      # is second-order coefficient (g/g^2)\n",
    "                               'K_3': 0.02         * 10**-6,      # is third-order coefficient  (g/g^3)\n",
    "                               'K_4': 0.006        * 10**-6,      # is fourth-order coefficient (g/g^4)\n",
    "                               'K_5': 0.0023       * 10**-6       # is fifth-order coefficient  (g/g^5)\n",
    "                               }\n",
    "        \n",
    "        ## Other acceleromter error coefficients that could be added in the future.\n",
    "        # self.K_0_asym = 0                   # Bias Asymmetry \n",
    "        # self.K_1_asym = 0                   # Scale Factor Asymmetry\n",
    "        # self.K_oq = 0                       # Odd Quadratic Coefficient\n",
    "        # self.omeg_o = 0                    # is misalignmet of the IA with respect to the OA\n",
    "        # self.omeg_p = 0                    # is misalignmen of the IA with respect to the PA\n",
    "        # self.K_ip = 0                      # is crosscoupling coefficient \n",
    "        # self.K_io = 0                      # is crosscoupling coefficient\n",
    "        # self.K_po = 0                      # is crosscoupling coefficient\n",
    "        # self.K_pp = 1.32E-4 * 10**-6       # is cross-axis nonlinearity coefficients\n",
    "        # self.K_ppp = 2.10E-7 * 10**-6\n",
    "        # self.K_pppp = 2.3E-10 * 10**-6\n",
    "        # self.K_oo = 0                      # is cros-axis nonlinearity coefficients\n",
    "        # self.K_spin = 0                    # is spin correction coefficient, equal to \n",
    "        # self.K_ang_accel = 0               # is angular acceleration coefficient\n",
    "        \n",
    "        \n",
    "    def simulate(self,a_i,n_start_idx, n_stop_idx):\n",
    "        \"\"\"\n",
    "        Starting with one dimensional error model. Outputs acceleration given\n",
    "        true input acceleration. In the future errors caused by inputs along the pendulus axis (a_p) \n",
    "        and output axis (a_o) \n",
    "        could be added.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert acceleration into g's since the error coefficients are defined in terms of g's. \n",
    "        g_i = a_i / self.g\n",
    "        \n",
    "        accel_model = [self.AccelModelCoef['K_1'] * (g_i),\n",
    "                       self.AccelModelCoef['K_0'] * np.ones(len(g_i)),  \n",
    "                       self.AccelModelCoef['K_2'] * (g_i**2), \n",
    "                       self.AccelModelCoef['K_3'] * (g_i**3), \n",
    "                       self.AccelModelCoef['K_4'] * (g_i**4), \n",
    "                       self.AccelModelCoef['K_5'] * (g_i**5)]\n",
    "        \n",
    "        # Add accelerometer error from each coefficient together and multiply by g then add original acceleration.\n",
    "        a_x_Sim = self.g * sum(accel_model[n_start_idx:n_stop_idx]) + a_i\n",
    "        \n",
    "        \n",
    "        return a_x_Sim\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16bfbf",
   "metadata": {},
   "source": [
    "## Generate Reference Trajectory\n",
    "### Description:\n",
    "These set of functions are used to generate a sled test trajectory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96a6d3",
   "metadata": {},
   "source": [
    "##### importEGIData(Headers,filepath)\n",
    "Imports data from a .csv file into columns titled using the inputed Headers list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76642fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importEGIData(Headers,filepath):\n",
    "        \n",
    "    if filepath == '':\n",
    "        print('No file selected')\n",
    "    else: \n",
    "        D = pd.read_csv(filepath , names = Headers) # Pull only first row from Excel File\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5b874",
   "metadata": {},
   "source": [
    "###### lpf(x, omege_c, T):\n",
    "This function filters inputted data using a first order low pass filter. This is used to smooth out accelerometer data collected from a real sled test for use as data to create the reference trajectory.\n",
    " - x = Input array.\n",
    " - omega_c = Cutoff frequency.\n",
    " - T = Sample time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpf(x, omega_c, T):\n",
    "    \"\"\"Implement a first-order low-pass filter.\n",
    "    \n",
    "    The input data is x, the filter's cutoff frequency is omega_c \n",
    "    [rad/s] and the sample time is T [s].  The output is y.\n",
    "    \"\"\"\n",
    "    N = np.size(x)\n",
    "    y = x\n",
    "    alpha = (2-T*omega_c)/(2+T*omega_c)\n",
    "    beta = T*omega_c/(2+T*omega_c)\n",
    "    for k in range(1, N):\n",
    "        y[k] = alpha*y[k-1] + beta*(x[k]+x[k-1])\n",
    "        \n",
    "    return y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963564dd",
   "metadata": {},
   "source": [
    "###### generateReferenceTrajectory()\n",
    "This function generates a reference trajectory given some inputted accleration data. For this implementation it takes in acceleration and velocity data from an EGI on board a sled test that took place at the HHSTT. \n",
    "\n",
    "generateReferenceTrajectory Steps:\n",
    "\n",
    "Step 1: Import Data -\n",
    "Real acceleration and velocity data was collected from an Embedded GPS/INS device mounted on a guidance sled test. All three axes of data is imported but only the values from the downtrack (X) axis is used in this implementation. \n",
    "\n",
    "Step 2: Cleaning data -\n",
    "The data is then trimmed to focus on the part of the sled test where the actual launch occurs. The EGI sat for hours prior to launch during calibration of the unit under test but this data isn't necessary for this investigation. Occasionaly the time series from the raw data had repeated times for sucessive data points and so a new time series with an even sample rate was created and aplied to the data set to create a more even reference trajectory. The new time series for the data was created by taking the total duration of the trajetory and dividing it by the total number of data points in the trajectory to get the new sample rate.\n",
    "\n",
    "Step 3: Smoothing Data - \n",
    "The data was then smoothed using a low pass filter to reduce high frequency noise and create a smoother trajectory to use as the reference trajectory.\n",
    "\n",
    "Step 4: Save Data to DataFrame - \n",
    "Oranize the data by saving it to a single pandas dataframe.\n",
    "\n",
    "Step 5: Create distance trajectories\n",
    "The reference distance trajectory is made by double integrating the down track acceleration from the EGI. Motion prior to and after sled motion was forced to be zero where the actual measurement had some noise. The start time of the trajectory was also set to 0 meaning all data occuring prior to sled motion occured at \"negative\" time. \n",
    "\n",
    "\n",
    "The Pandas DataFrame with all the data stored is saved as a pickle file to be imported and used later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be115374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateReferenceTrajectory(plotcheck = False):\n",
    "    \n",
    "    # Define the file paths for acceleration and velocity data. \n",
    "    Accel_filepath = './EGI_data/EGI_accel.csv'\n",
    "    Vel_filepath = './EGI_data/EGI_accel.csv'\n",
    "    \n",
    "    # Save data into Pandas data frames with defined headers.\n",
    "    EGI_accel = importEGIData(['Time', 'Ax','Ay','Az'],Accel_filepath)\n",
    "    EGI_vel = importEGIData(['Time', 'Vx','Vy', 'Vz'],Vel_filepath)\n",
    "\n",
    "    # Combine the Acceleration and Velocity data frames into one.\n",
    "    EGI_accel_vel = EGI_accel.join(EGI_vel[['Vx','Vy','Vz']])\n",
    "\n",
    "    ###############################################################################################################\n",
    "    # Truth Gen Step 2 - Clean Data\n",
    "    # Trim data to focus on actual sled run. These points were determined visually from the data used. \n",
    "    print('Developing Reference Trajectory')\n",
    "    print(\"Trimming data to start/stop time determined visually...\")\n",
    "    startTime = 399600   # Index of data of the beginning of the reference trajectory\n",
    "    stopTime = 399700    # Index of data of the end of the reference tracjectory\n",
    "\n",
    "    # Trim the reference trajectory to the start and stop indicies defined above.  \n",
    "    EGI_accel_vel_trim = EGI_accel_vel[(EGI_accel_vel['Time'] > startTime) & (EGI_accel_vel['Time'] < stopTime) ] # trim accelerometer output\n",
    "\n",
    "    # The data used for creating the reference trajectory had repeated time values for multiple measurements of velocitty and acceleration.\n",
    "    # To create a smooth reference trajectory the below code creates a new time series for the data by taking the total duration of the trajetory\n",
    "    # and dividing it by the total number of data points in the trajectory so the new sample rate is even across the whole trajetory.\n",
    "    \n",
    "    # Determine new time series parameters\n",
    "    # Tdur = total duration of data in seconds.\n",
    "    # Tlen = number of data points\n",
    "    Tdur = EGI_accel_vel_trim['Time'].max() - EGI_accel_vel_trim['Time'].min() \n",
    "    Tlen = len(EGI_accel_vel_trim['Time'])\n",
    "\n",
    "    # Generate new time series given duration of trajectory and number of data points.\n",
    "    NewTimeSeries = np.linspace(0, Tdur, Tlen)\n",
    "    \n",
    "    # Save new time series to Data Frame.\n",
    "    EGI_accel_vel_trim.loc[:,'New Time'] = NewTimeSeries\n",
    "     \n",
    "    ###############################################################################################################    \n",
    "    #%% Truth Gen Step 3 - Smooth Acceleration in X-axis\n",
    "    # Pull data from data frame\n",
    "    EGI_accel_presmoothed = EGI_accel_vel_trim[['Ax']]\n",
    "    EGI_accel_smoothed_array = lpf(EGI_accel_vel_trim[['Ax']].to_numpy(),50,Tdur/Tlen)\n",
    "    EGI_accel_vel_trim['Original_Ax'] = EGI_accel_presmoothed\n",
    "\n",
    "    ###############################################################################################################\n",
    "    #%% Truth Gen Step 4 - Create a DataFrame to house all truth data\n",
    "    referenceTrajectory = pd.DataFrame()\n",
    "\n",
    "    referenceTrajectory['Time'] = EGI_accel_vel_trim['New Time']\n",
    "    referenceTrajectory['refAccel_x'] = EGI_accel_smoothed_array\n",
    "    referenceTrajectory['refEGIVel_x'] = EGI_accel_vel_trim['Vx']\n",
    "\n",
    "    ###############################################################################################################\n",
    "    #%% Truth Gen Step 5 - Create distance trajectories.\n",
    "    # Change initial acceleration in X to zero until launch. Determined visually\n",
    "    print(\"Setting initial acceleration to 0 until launch...\")\n",
    "    referenceTrajectory['refAccel_x'][:1145] = 0\n",
    "\n",
    "    # Change final acceleration after stop to zero. Determined visually\n",
    "    print(\"Setting final acceleration at 0...\")\n",
    "    referenceTrajectory['refAccel_x'][4968:] = 0\n",
    "    \n",
    "    #%% Truth Gen Step 6 -  Integrate truth acceleration to get velocity and distance\n",
    "    referenceTrajectory['refVel_x'] = integrate.cumulative_trapezoid(y = referenceTrajectory['refAccel_x'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    \n",
    "    # Change final Velocity after stop to zero. Determined visually\n",
    "    print(\"Setting final velocity at 0...\")\n",
    "    referenceTrajectory['refVel_x'][4968:] = 0\n",
    "    \n",
    "    # Integrate velocity to get trajectory distance. \n",
    "    referenceTrajectory['refDist_x'] = integrate.cumulative_trapezoid(y = referenceTrajectory['refVel_x'],x = referenceTrajectory['Time'],initial = 0) \n",
    "\n",
    "    # Integrate EGI velocity to compare to double integrated acceleration\n",
    "    referenceTrajectory['refEGIDist_x'] = integrate.cumulative_trapezoid(y = referenceTrajectory['refEGIVel_x'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    \n",
    "    # Compute start motion time.\n",
    "    startMotionTime = referenceTrajectory['Time'][referenceTrajectory['refAccel_x']>0.001].iloc[0]\n",
    "    # Set the start motion time as 0. \n",
    "    referenceTrajectory['Time'] = referenceTrajectory['Time']-startMotionTime\n",
    "    \n",
    "    \n",
    "    #%% Save trajectory to Pickle File   \n",
    "    referenceTrajectory.to_pickle(\"./referenceTrajectory.pkl\")\n",
    "    \n",
    "    #%% Plots Acceleration and Velocity\n",
    "    if plotcheck == True:\n",
    "        Figure1 = PlotlyPlot()\n",
    "        Figure1.setTitle('EGI Acceleration, Velocity and Smoothed acceleration')\n",
    "        Figure1.setYaxisTitle('Acceleration (m/s/s)')\n",
    "        Figure1.setYaxis2Title('Velocity (m/s)')\n",
    "        Figure1.setXaxisTitle('GPS Time (s)')\n",
    "        Figure1.settwoAxisChoice([False, True])\n",
    "        Figure1.plotTwoAxis(referenceTrajectory[['Ax','Vx']], df_x= EGI_accel_vel_trim[['New Time']])\n",
    "        Figure1.addLine(referenceTrajectory[['refAccel_x']], df_x = referenceTrajectory[['Time']],secondary_y=False)\n",
    "        Figure1.show()\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1a59d",
   "metadata": {},
   "source": [
    "#### generateTrackRPV()\n",
    "\n",
    "This function generates a reference position vector given a given reference trajector as created by generateReferenceTrajectory()\n",
    "\n",
    "\n",
    "Step 1: Set up interupter system.  \n",
    "The track reference is generated using interupter blades that are located approximately every 4.5 feet down the track. The system on the guidance sled records the time in which every interupter is passed and is then processed so that a downtrack distance of the sled is associated with every interupt.\n",
    "\n",
    "The interupters are located at fixed downtrack distances so the reference trajectory can be used to determine at what time each interupter would be passed.\n",
    "\n",
    "$$t=t_1+\\left(I-R(t_1)\\right) \\frac{t_2-t_1}{R(t_2)-R(t_1)}$$\n",
    "\n",
    "$R(t)$ = Reference Trajectory Down Track Distance at time $t$  \n",
    "$T$ = Reference Tracjectory Time  \n",
    "$I$ = Interupter Down Track Distance that falls between $R(t_2)$ and $R(t_1)$  \n",
    "$t$ = Time\n",
    "\n",
    "Step 2: Adding Error to Reference Position Vector  \n",
    "This code has the ability to add a variety of errors to the RPV. Most notably the ability to add random noise given.\n",
    "\n",
    "Noise is added using the following code (Zero mean, normal noise).  \n",
    "    ```noise = np.random.normal(0,sigmaRPV,len(trackRPV)) # Add random noise to RPV```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6104abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTrackRPV(referenceTrajectory, sigmaRPV, tauRPV, biasRPV, Overwrite=True):\n",
    "    \n",
    "    # print(\"\\n Generating RPV...\")\n",
    "    trackRPV = pd.DataFrame()\n",
    "        \n",
    "    Interupter_delta = 4.5 * 0.3048 # ft converted to meters\n",
    "    TrackLength = 10000   # Meters  \n",
    "    \n",
    "    trackRPV['Interupters_DwnTrk_dist'] = np.arange(0, TrackLength, Interupter_delta)\n",
    "    trackRPV['Time'] = np.interp(trackRPV['Interupters_DwnTrk_dist'],referenceTrajectory['refDist_x'],referenceTrajectory['Time'])\n",
    "    \n",
    "    # Trim off the RPV so that there is no interupts past the time the sled stopped motion.\n",
    "    trackRPV = trackRPV[trackRPV['Interupters_DwnTrk_dist'] <= referenceTrajectory['refDist_x'].max()]\n",
    "    trackRPV = trackRPV.drop_duplicates(subset=['Time'])\n",
    "    trackRPV = trackRPV[:-1]\n",
    "    \n",
    "    ###########################################################################################################\n",
    "    #\n",
    "    # REMOVED ZERO VELOCITY CODE. \n",
    "    # Code can be found in original python code. Remove\n",
    "    # because adding areas of the trajectory where motion was\n",
    "    # zero seemed to hurt estimates. \n",
    "    #\n",
    "    ###########################################################################################################\n",
    "    \n",
    "    # Sort the values by time.\n",
    "    trackRPV = trackRPV.sort_values(by='Time').reset_index(drop=True)\n",
    "        \n",
    "    # Add error to Track RPV\n",
    "    if sigmaRPV != 0:\n",
    "        noise = np.random.normal(0,sigmaRPV,len(trackRPV)) # Add random noise to RPV\n",
    "        trackRPV['Interupters_DwnTrk_dist'] = trackRPV['Interupters_DwnTrk_dist'] + noise\n",
    "    \n",
    "    if tauRPV != 0: \n",
    "        trackRPV['Time'] = trackRPV['Time'] - tauRPV # Add time lag error\n",
    "        \n",
    "    if biasRPV != 0:\n",
    "        trackRPV['Interupters_DwnTrk_dist'] = trackRPV['Interupters_DwnTrk_dist'] + biasRPV # Add distance bias.\n",
    "\n",
    "    \n",
    "    ###########################################################################################################\n",
    "    #%% Save track RPV to pickle file\n",
    "    if Overwrite == True:\n",
    "        trackRPV.to_pickle(f\"./RPVs/trackRPV_sig{sigmaRPV}_tau{tauRPV}_bias{biasRPV}.pkl\")\n",
    "    else:\n",
    "       filepath = incrementFileName(f\"./VarianceRPVs/trackRPV_sig{sigmaRPV}_tau{tauRPV}_bias{biasRPV}.pkl\")\n",
    "       trackRPV.to_pickle(filepath)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916c07f",
   "metadata": {},
   "source": [
    "#### AccelSim()\n",
    "\n",
    "ACCEL SIM - Scripts used to generate simulated accelerometer output based on truth input. Uses smoothed acceleration reference trajectory data to simulate. The simulated acceleration, velocity and distance is saved to a PandasData frame.\n",
    "\n",
    "This method uses scipy's ```cumulative_trapezoid()``` to integrate acceleration to get velocity and distance. NOTE: This could potentially be improved with better methods.\n",
    "\n",
    "\n",
    "See [Accelerometer Class](#Accelerometer_Class) for details on the acceleromter error model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AccelSim(referenceTrajectory, N_model, changeDefaultCoeff, CoeffDict, g):\n",
    "    \n",
    "    #%% ACCEL SIM Step 1 - Simulate a Acceleromter with Bias using Accelerometer class\n",
    "    \"\"\"\n",
    "    ACCEL SIM - Scripts used to generate simulated accelerometer output based on truth input\n",
    "    \n",
    "    Using smoothed acceleration truth data to simulate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Intialize accelerometer class.\n",
    "    AccelOne = Accelerometer()\n",
    "    \n",
    "    # Change the default error model coeficients defined in the Accleromter class. \n",
    "    if changeDefaultCoeff == True:\n",
    "            AccelOne.AccelModelCoef.update(CoeffDict)\n",
    "    \n",
    "    # Create data frame to house data\n",
    "    sensorSim = pd.DataFrame()\n",
    "    sensorSim['Time'] = referenceTrajectory['Time']\n",
    "    \n",
    "    # Change to array for use in simulation.\n",
    "    A_i_true = referenceTrajectory['refAccel_x'].to_numpy()  \n",
    "    \n",
    "    # Simulate\n",
    "    A_x_sim = AccelOne.simulate(A_i_true, N_model[0], N_model[1])  \n",
    "    \n",
    "    # Store data in data frame. \n",
    "    sensorSim['SensorSim_Ax'] = A_x_sim\n",
    "    \n",
    "    # Accelerometer data is set to 0 prior to first motion. \n",
    "    sensorSim['SensorSim_Ax'][referenceTrajectory['refAccel_x'] == 0] = 0\n",
    "    \n",
    "    #%% Integrate Simulated accelerations to develop Velocity and Displacement.\n",
    "    sensorSim['SensorSim_Vx'] = integrate.cumulative_trapezoid(y = sensorSim['SensorSim_Ax'],x = sensorSim['Time'],initial = 0) \n",
    "    sensorSim['SensorSim_Dx'] = integrate.cumulative_trapezoid(y = sensorSim['SensorSim_Vx'],x = sensorSim['Time'],initial = 0) \n",
    "    \n",
    "    AccelObj = AccelOne\n",
    "    \n",
    "    return [sensorSim, AccelObj]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa11ed1",
   "metadata": {},
   "source": [
    "#### RegressionAnalysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402e15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegressionAnalysis(referenceTrajectory, trackRPV, AccelObj, sensorSim, N_model, g,sigmaRPV, saveToPickel = False, WLSoption = True, LeastSquaresMethod = 'LongHand',computeCovariance = True):\n",
    "\n",
    "    \"\"\"\n",
    "    Error - Scripts used to compare accelerometer simulation versus track truth\n",
    "    \"\"\"\n",
    "    Dist_Error = pd.DataFrame()\n",
    "    Dist_Error['Time'] = trackRPV['Time']\n",
    "    \n",
    "    ##############################################################################################################\n",
    "    # Step 1: Interpolate Sensor Sim to Track \n",
    "    ##############################################################################################################\n",
    "\n",
    "    trackRPV['SensorDwnTrkDist'] = np.interp(trackRPV['Time'],sensorSim['Time'],sensorSim['SensorSim_Dx'])    \n",
    "    Dist_Error['DistErr_x'] = trackRPV['Interupters_DwnTrk_dist'] - trackRPV['SensorDwnTrkDist']\n",
    "    \n",
    "    ##############################################################################################################\n",
    "    # Step 2: Compute Velocity Error\n",
    "    ##############################################################################################################\n",
    "    # Compute Velocity Error\n",
    "    Ve_x = (np.diff(Dist_Error['DistErr_x'])/np.diff(Dist_Error['Time']))\n",
    "    Ve_t = (Dist_Error['Time'].head(-1) + np.diff(Dist_Error['Time'])/2).to_numpy()\n",
    "    \n",
    "    Error = pd.DataFrame()\n",
    "    \n",
    "    Error['Time'] = Ve_t\n",
    "    Error['SensorSim_Ax'] = np.interp(Ve_t,sensorSim['Time'],sensorSim['SensorSim_Ax']) \n",
    "    Error['SensorSim_Vx'] = np.interp(Ve_t,sensorSim['Time'],sensorSim['SensorSim_Vx'])\n",
    "    Error['SensorSim_Dx'] = np.interp(Ve_t,sensorSim['Time'],sensorSim['SensorSim_Dx'])\n",
    "    Error['DistErr_x'] = np.interp(Ve_t,Dist_Error['Time'],Dist_Error['DistErr_x']) \n",
    "    Error['VelErr_x'] = Ve_x\n",
    "     \n",
    "    #%% - Regression Analysis\n",
    "    \"\"\"\n",
    "    Regression Analysis - Scripts used to compute error model\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Compute coordinate functions\n",
    "    referenceTrajectory['Ax^2 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**2\n",
    "    referenceTrajectory['Ax^3 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**3\n",
    "    referenceTrajectory['Ax^4 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**4\n",
    "    referenceTrajectory['Ax^5 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**5\n",
    "    \n",
    "    referenceTrajectory['intAx^2 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^2 (g)'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    referenceTrajectory['intAx^3 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^3 (g)'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    referenceTrajectory['intAx^4 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^4 (g)'],x = referenceTrajectory['Time'],initial = 0)\n",
    "    referenceTrajectory['intAx^5 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^5 (g)'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    \n",
    "    \n",
    "    Vx = np.interp(Ve_t, referenceTrajectory['Time'],referenceTrajectory['refVel_x'])\n",
    "    intAx_2 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^2 (g)']) \n",
    "    intAx_3 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^3 (g)']) \n",
    "    intAx_4 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^4 (g)']) \n",
    "    intAx_5 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^5 (g)'])\n",
    "    \n",
    "    coordinateFunctionDF = pd.DataFrame()\n",
    "    coordinateFunctionDF['Time'] = Ve_t\n",
    "     \n",
    "    coeff_dict = {'Est_V_0': 0, 'Est_K_1': 0, 'Est_K_0': 0, 'Est_K_2': 0, 'Est_K_3': 0, 'Est_K_4': 0, 'Est_K_5': 0}\n",
    "    \n",
    "    # Create Complete A Matrix\n",
    "    complete_A = np.array([np.ones(len(Ve_t))/g, -Vx/g, -Ve_t, intAx_2, intAx_3, intAx_4, intAx_5])*g\n",
    "    complete_A = complete_A.T\n",
    "    \n",
    "    complete_A_DF = pd.DataFrame(np.fliplr(complete_A), columns=['IntAx_5', 'IntAx_4', 'IntAx_3', 'IntAx_2', 'Ve_t', 'Vx', 'Ones'])\n",
    "    \n",
    "    trimmed_A_filt = np.zeros(complete_A.shape[1], dtype = bool)\n",
    "    trimmed_A_filt[0] = 1\n",
    "    \n",
    "    trimmed_A_filt[N_model[0]+1:N_model[1]+1] = 1\n",
    "\n",
    "    trimmed_A = complete_A[:,trimmed_A_filt]\n",
    "    A = trimmed_A\n",
    "    \n",
    "    '''\n",
    "    COMPUTE COVARIANCE\n",
    "    '''\n",
    "    #%% \n",
    "    # Linear Regression\n",
    "    coeff_list = tuple(None for _ in range(trimmed_A.shape[1]))\n",
    "\n",
    "    if sigmaRPV == 0 or WLSoption == False: \n",
    "        size = trimmed_A.shape[0]\n",
    "        W = np.identity(size)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        ################################################################################################\n",
    "        # Develop weighted matrix\n",
    "        ################################################################################################\n",
    "        # Compute Velocity Error Uncertainty\n",
    "        delta_t = np.diff(trackRPV['Time'])\n",
    "        vel_sig = np.sqrt(2)*sigmaRPV/delta_t\n",
    "        \n",
    "        # Compute Velocity Error Variance\n",
    "        vel_var = np.square(vel_sig)\n",
    "        \n",
    "        # Computed Weighted Least Squares Weighting Matrix\n",
    "        w = np.diag(vel_var,0) - np.diag((.5*vel_var[1:]),-1) - np.diag((.5*vel_var[1:]),1)    \n",
    "        W = np.linalg.inv(w)\n",
    "        # W = np.diag(1/vel_sig) \n",
    "    \n",
    "\n",
    "    \n",
    "    AW = np.transpose(trimmed_A).dot(W)\n",
    "    Ve_xW = W.dot(Ve_x)\n",
    "    \n",
    "    if LeastSquaresMethod == 'Numpy':\n",
    "        coeff_list = np.linalg.lstsq(np.transpose(AW), Ve_xW, rcond=None)[0] # This has just been used for debugging to check if \"Long\" least squares leads to same results.\n",
    "    elif LeastSquaresMethod == 'SciKit':\n",
    "        testSKlearn = LinearRegression()\n",
    "        testSKlearn.fit(trimmed_A, Ve_x, sample_weight=(np.diag(W)))\n",
    "        coeff_list = testSKlearn.coef_\n",
    "        coeff_list[0] = testSKlearn.intercept_\n",
    "    elif LeastSquaresMethod == 'LongHand':\n",
    "        At = np.transpose(trimmed_A)\n",
    "        coeff_list = np.linalg.inv(At.dot(W).dot(trimmed_A)).dot(At).dot(W).dot(Ve_x)\n",
    "        \n",
    "    else: \n",
    "        print(\"Did not select an applicable Least Squares Method\")\n",
    "\n",
    "    ################################################################################################\n",
    "    # Compute Covariance Matrix\n",
    "    ################################################################################################    \n",
    "    # Compute Covariance  \n",
    "    if computeCovariance == True:\n",
    "        covariance_A = np.linalg.inv(At.dot(W).dot(A))\n",
    "    else:\n",
    "        covariance_A = 'Covariance Not Computed'\n",
    "     \n",
    "    #######################\n",
    "    # USED FOR DEBUGGING\n",
    "    #######################\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    print_List = np.array(list(coeff_dict.keys()))\n",
    "    \n",
    "    n = 0\n",
    "    for coef in print_List[trimmed_A_filt]:\n",
    "        coeff_dict[coef] = coeff_list[n]\n",
    "        n += 1\n",
    "    \n",
    "    #%% Save results to DataFrame\n",
    "    coefficientDF = pd.DataFrame()\n",
    "    \n",
    "    coefficientDF = pd.concat((coefficientDF, pd.DataFrame.from_dict(AccelObj.AccelModelCoef, orient = 'index', columns= ['Accel Model'])))\n",
    "    coefficientDF.loc['V_0'] = 0\n",
    "    \n",
    "    # Build Estimated Coefficient DF\n",
    "    estimatedCoefficients = pd.DataFrame.from_dict(coeff_dict, orient = 'index', columns= ['Estimated Coefficients'])\n",
    "    renameDict = {}\n",
    "    for coeff in print_List:\n",
    "        renameDict[coeff] = coeff[4:]\n",
    "    estimatedCoefficients = estimatedCoefficients.rename(index = renameDict) \n",
    "    estimatedCoefficients.replace(0, np.nan, inplace=True)\n",
    "    \n",
    "    \n",
    "    coefficientDF = pd.merge(coefficientDF,estimatedCoefficients,left_index=True, right_index=True)\n",
    "    \n",
    "    coefficientDF['Coefficient Estimate Error'] = coefficientDF['Accel Model'] - coefficientDF['Estimated Coefficients']\n",
    "    \n",
    "                \n",
    "    #%% Compute Velocity Error Residuals\n",
    "    V_error_model_terms = [coeff_dict['Est_V_0'], \n",
    "                           coeff_dict['Est_K_1']*Vx,  \n",
    "                           coeff_dict['Est_K_0']*Ve_t, \n",
    "                           coeff_dict['Est_K_2']*intAx_2, \n",
    "                           coeff_dict['Est_K_3']*intAx_3,  \n",
    "                           coeff_dict['Est_K_4']*intAx_4,  \n",
    "                           coeff_dict['Est_K_5']*intAx_5]\n",
    "    Error['V_error_model'] = sum(V_error_model_terms)*g \n",
    "    Error['Ve_x_Resid'] = Error['VelErr_x'] - Error['V_error_model']     \n",
    "\n",
    "    #%% Save off results:\n",
    "    if saveToPickel == True:\n",
    "        Error.to_pickle(f\"./ErrorDF_{N_model[0]}-{N_model[1]}.pkl\")\n",
    "        coefficientDF.to_pickle(f\"./coefficientDF_{N_model[0]}-{N_model[1]}.pkl\")\n",
    "        \n",
    "    return [coefficientDF, Error, covariance_A, A, Ve_x, W, LeastSquaresMethod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e75843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
