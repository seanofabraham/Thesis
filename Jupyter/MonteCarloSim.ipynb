{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bae2e48",
   "metadata": {},
   "source": [
    "# Determining the Effectiveness of the High Speed Test Track testing for the determination of accelerometer error coefficients. \n",
    "### By Sean Abrahamson \n",
    "\n",
    "This is a jupyter note book walking through the code for simulating an the performace of a single accelerometer output going down the Holloman High Speed Test Track and then using a the 746 TS Reference Position Vector to computer the error coefficients using least squares. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d571973",
   "metadata": {},
   "source": [
    "### Import necessary libaries and functions from custom functions and classes from other jupiter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9756bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import plotly.graph_objects as go\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# from classes_x import *\n",
    "from scipy import integrate\n",
    "from sigfig import round\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'browser'\n",
    "\n",
    "import pdb\n",
    "\n",
    "# Import other Jupyter Notebooks\n",
    "%run Thesis_Utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583d6c3",
   "metadata": {},
   "source": [
    "<a id='Accelerometer_Class'></a>\n",
    "\n",
    "## Accelerometer Class\n",
    "\n",
    "### Description:\n",
    "The acceleromerter class defines an accelerometer object that contains an error model and a simulate function. The simulate function applies the associated error model attribute of the accelerombeter object and outputs what the output of the accelerometer would be given specific acceleration inputs. \n",
    "#### ___init___(self):\n",
    "The accelerometer class is initiatlized with an error model that determines the kind of error the accelerometer demonstrates. For the purpose of this effort only the scale-factor non-linearity terms were added as they are a major focus of recent sled testing efforts. These values are estimated values for strategic grade resonating beam accelerometers. The values below can be found in the table below. \n",
    "\n",
    "| Coefficient\t|Value\t      |Units\t       |Description                          |\n",
    "| :---          | :---:       | :---:          | :---                                |\n",
    "|$K_0$\t        |5\t          |$\\mu g$\t             |Bias                                 |\n",
    "|$K_1$\t\t    |.005         |$\\mu g/g $\t         |Scale Factor Error                   |\n",
    "|$K_2$\t        |60.144       |$\\mu \\frac{g}{g^2}$   |Scale factor 2nd order non-linearity |\n",
    "|$K_3$\t        |0.0152\t      |$\\mu \\frac{g}{g^3}$   |Scale factor 3rd order non-linearity |\n",
    "|$K_4$\t        |0.0058\t      |$\\mu \\frac{g}{g^4}$   |Scale factor 4th order non-linearity |\n",
    "|$K_5$\t        |0.0023       |$\\mu \\frac{g}{g^5}$   |Scale factor 5th order non-linearity |\n",
    "\n",
    "\n",
    "#### function *simulate(self, a_i, n_start_idx, n_stop_idx)*\n",
    "\n",
    "##### Description: \n",
    "The simulate function simulates the output of a single acceleromter given accelation $(A_i)$ in g's along it's input axis. The accelerometer error $(A_{err})$ is given by the below equation\n",
    "\n",
    "$$A_{err}= K_0+K_1 A_{i}+K_2A_{i}^{2}+K_3A_{i}^{3}+K_4A_{i}^{4}+K_5A_{i}^{5}$$\n",
    "\n",
    "To get the actual output of the acceleromter the computed error is converted to ($m/s^2$) then added to the original input acceleration\n",
    "\n",
    "$$A_{sim} = g*(A_{err}) + A_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86096a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accelerometer:\n",
    "    \n",
    "    def __init__(self):  # Default accelerometer characteristics\n",
    "     \n",
    "        self.g = 9.791807                     # Definition of g\n",
    "        \n",
    "        self.AccelModelCoef = {'K_1': 5            * 10**-6,      # Scale Factor (g/g) NEEDS UPDATED\n",
    "                               'K_0': .005         * 10**-6,      # Bias (g)\n",
    "                               'K_2': 61.14        * 10**-6,      # is second-order coefficient (g/g^2)\n",
    "                               'K_3': 0.02         * 10**-6,      # is third-order coefficient  (g/g^3)\n",
    "                               'K_4': 0.006        * 10**-6,      # is fourth-order coefficient (g/g^4)\n",
    "                               'K_5': 0.0023       * 10**-6       # is fifth-order coefficient  (g/g^5)\n",
    "                               }\n",
    "        \n",
    "        ## Other acceleromter error coefficients that could be added in the future.\n",
    "        # self.K_0_asym = 0                   # Bias Asymmetry \n",
    "        # self.K_1_asym = 0                   # Scale Factor Asymmetry\n",
    "        # self.K_oq = 0                       # Odd Quadratic Coefficient\n",
    "        # self.omeg_o = 0                    # is misalignmet of the IA with respect to the OA\n",
    "        # self.omeg_p = 0                    # is misalignmen of the IA with respect to the PA\n",
    "        # self.K_ip = 0                      # is crosscoupling coefficient \n",
    "        # self.K_io = 0                      # is crosscoupling coefficient\n",
    "        # self.K_po = 0                      # is crosscoupling coefficient\n",
    "        # self.K_pp = 1.32E-4 * 10**-6       # is cross-axis nonlinearity coefficients\n",
    "        # self.K_ppp = 2.10E-7 * 10**-6\n",
    "        # self.K_pppp = 2.3E-10 * 10**-6\n",
    "        # self.K_oo = 0                      # is cros-axis nonlinearity coefficients\n",
    "        # self.K_spin = 0                    # is spin correction coefficient, equal to \n",
    "        # self.K_ang_accel = 0               # is angular acceleration coefficient\n",
    "        \n",
    "        \n",
    "    def simulate(self,a_i,n_start_idx, n_stop_idx):\n",
    "        \"\"\"\n",
    "        Starting with one dimensional error model. Outputs acceleration given\n",
    "        true input acceleration. In the future errors caused by inputs along the pendulus axis (a_p) \n",
    "        and output axis (a_o) \n",
    "        could be added.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert acceleration into g's since the error coefficients are defined in terms of g's. \n",
    "        g_i = a_i / self.g\n",
    "        \n",
    "        accel_model = [self.AccelModelCoef['K_1'] * (g_i),\n",
    "                       self.AccelModelCoef['K_0'] * np.ones(len(g_i)),  \n",
    "                       self.AccelModelCoef['K_2'] * (g_i**2), \n",
    "                       self.AccelModelCoef['K_3'] * (g_i**3), \n",
    "                       self.AccelModelCoef['K_4'] * (g_i**4), \n",
    "                       self.AccelModelCoef['K_5'] * (g_i**5)]\n",
    "        \n",
    "        # Add accelerometer error from each coefficient together and multiply by g then add original acceleration.\n",
    "        a_x_Sim = self.g * sum(accel_model[n_start_idx:n_stop_idx]) + a_i\n",
    "        \n",
    "        \n",
    "        return a_x_Sim\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e55baa",
   "metadata": {},
   "source": [
    "## Generate Reference Trajectory\n",
    "### Description:\n",
    "These set of functions are used to generate a sled test trajectory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17165a",
   "metadata": {},
   "source": [
    "##### importEGIData(Headers,filepath)\n",
    "Imports data from a .csv file into columns titled using the inputed Headers list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34bb7c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importEGIData(Headers,filepath):\n",
    "        \n",
    "    if filepath == '':\n",
    "        print('No file selected')\n",
    "    else: \n",
    "        D = pd.read_csv(filepath , names = Headers) # Pull only first row from Excel File\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607d67d",
   "metadata": {},
   "source": [
    "#### lpf(x, omege_c, T):\n",
    "This function filters inputted data using a first order low pass filter. This is used to smooth out accelerometer data collected from a real sled test for use as data to create the reference trajectory.\n",
    " - x = Input array.\n",
    " - omega_c = Cutoff frequency.\n",
    " - T = Sample time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c99880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpf(x, omega_c, T):\n",
    "    \"\"\"Implement a first-order low-pass filter.\n",
    "    \n",
    "    The input data is x, the filter's cutoff frequency is omega_c \n",
    "    [rad/s] and the sample time is T [s].  The output is y.\n",
    "    \"\"\"\n",
    "    N = np.size(x)\n",
    "    y = x\n",
    "    alpha = (2-T*omega_c)/(2+T*omega_c)\n",
    "    beta = T*omega_c/(2+T*omega_c)\n",
    "    for k in range(1, N):\n",
    "        y[k] = alpha*y[k-1] + beta*(x[k]+x[k-1])\n",
    "        \n",
    "    return y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f844562",
   "metadata": {},
   "source": [
    "#### generateReferenceTrajectory()\n",
    "This function generates a reference trajectory given some inputted accleration data. For this implementation it takes in acceleration and velocity data from an EGI on board a sled test that took place at the HHSTT. \n",
    "\n",
    "generateReferenceTrajectory Steps:\n",
    "\n",
    "Step 1: Import Data -\n",
    "Real acceleration and velocity data was collected from an Embedded GPS/INS device mounted on a guidance sled test. All three axes of data is imported but only the values from the downtrack (X) axis is used in this implementation. \n",
    "\n",
    "Step 2: Cleaning data -\n",
    "The data is then trimmed to focus on the part of the sled test where the actual launch occurs. The EGI sat for hours prior to launch during calibration of the unit under test but this data isn't necessary for this investigation. Occasionaly the time series from the raw data had repeated times for sucessive data points and so a new time series with an even sample rate was created and aplied to the data set to create a more even reference trajectory. The new time series for the data was created by taking the total duration of the trajetory and dividing it by the total number of data points in the trajectory to get the new sample rate.\n",
    "\n",
    "Step 3: Smoothing Data - \n",
    "The data was then smoothed using a low pass filter to reduce high frequency noise and create a smoother trajectory to use as the reference trajectory.\n",
    "\n",
    "Step 4: Save Data to DataFrame - \n",
    "Oranize the data by saving it to a single pandas dataframe.\n",
    "\n",
    "Step 5: Create distance trajectories\n",
    "The reference distance trajectory is made by double integrating the down track acceleration from the EGI. Motion prior to and after sled motion was forced to be zero where the actual measurement had some noise. The start time of the trajectory was also set to 0 meaning all data occuring prior to sled motion occured at \"negative\" time. \n",
    "\n",
    "\n",
    "The Pandas DataFrame with all the data stored is saved as a pickle file to be imported and used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f6d22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateReferenceTrajectory(plotcheck = False):\n",
    "    \n",
    "    # Define the file paths for acceleration and velocity data. \n",
    "    Accel_filepath = './EGI_data/EGI_accel.csv'\n",
    "    Vel_filepath = './EGI_data/EGI_accel.csv'\n",
    "    \n",
    "    # Save data into Pandas data frames with defined headers.\n",
    "    EGI_accel = importEGIData(['Time', 'Ax','Ay','Az'],Accel_filepath)\n",
    "    EGI_vel = importEGIData(['Time', 'Vx','Vy', 'Vz'],Vel_filepath)\n",
    "\n",
    "    # Combine the Acceleration and Velocity data frames into one.\n",
    "    EGI_accel_vel = EGI_accel.join(EGI_vel[['Vx','Vy','Vz']])\n",
    "\n",
    "    ###############################################################################################################\n",
    "    # Truth Gen Step 2 - Clean Data\n",
    "    # Trim data to focus on actual sled run. These points were determined visually from the data used. \n",
    "    print('Developing Reference Trajectory')\n",
    "    print(\"Trimming data to start/stop time determined visually...\")\n",
    "    startTime = 399600   # Index of data of the beginning of the reference trajectory\n",
    "    stopTime = 399700    # Index of data of the end of the reference tracjectory\n",
    "\n",
    "    # Trim the reference trajectory to the start and stop indicies defined above.  \n",
    "    EGI_accel_vel_trim = EGI_accel_vel[(EGI_accel_vel['Time'] > startTime) & (EGI_accel_vel['Time'] < stopTime) ] # trim accelerometer output\n",
    "\n",
    "    # The data used for creating the reference trajectory had repeated time values for multiple measurements of velocitty and acceleration.\n",
    "    # To create a smooth reference trajectory the below code creates a new time series for the data by taking the total duration of the trajetory\n",
    "    # and dividing it by the total number of data points in the trajectory so the new sample rate is even across the whole trajetory.\n",
    "    \n",
    "    # Determine new time series parameters\n",
    "    # Tdur = total duration of data in seconds.\n",
    "    # Tlen = number of data points\n",
    "    Tdur = EGI_accel_vel_trim['Time'].max() - EGI_accel_vel_trim['Time'].min() \n",
    "    Tlen = len(EGI_accel_vel_trim['Time'])\n",
    "\n",
    "    # Generate new time series given duration of trajectory and number of data points.\n",
    "    NewTimeSeries = np.linspace(0, Tdur, Tlen)\n",
    "    \n",
    "    # Save new time series to Data Frame.\n",
    "    EGI_accel_vel_trim.loc[:,'New Time'] = NewTimeSeries\n",
    "     \n",
    "    ###############################################################################################################    \n",
    "    #%% Truth Gen Step 3 - Smooth Acceleration in X-axis\n",
    "    # Pull data from data frame\n",
    "    EGI_accel_presmoothed = EGI_accel_vel_trim[['Ax']]\n",
    "    EGI_accel_smoothed_array = lpf(EGI_accel_vel_trim[['Ax']].to_numpy(),50,Tdur/Tlen)\n",
    "    EGI_accel_vel_trim['Original_Ax'] = EGI_accel_presmoothed\n",
    "\n",
    "    ###############################################################################################################\n",
    "    #%% Truth Gen Step 4 - Create a DataFrame to house all truth data\n",
    "    referenceTrajectory = pd.DataFrame()\n",
    "\n",
    "    referenceTrajectory['Time'] = EGI_accel_vel_trim['New Time']\n",
    "    referenceTrajectory['refAccel_x'] = EGI_accel_smoothed_array\n",
    "    referenceTrajectory['refEGIVel_x'] = EGI_accel_vel_trim['Vx']\n",
    "\n",
    "    ###############################################################################################################\n",
    "    #%% Truth Gen Step 5 - Create distance trajectories.\n",
    "    # Change initial acceleration in X to zero until launch. Determined visually\n",
    "    print(\"Setting initial acceleration to 0 until launch...\")\n",
    "    referenceTrajectory['refAccel_x'][:1145] = 0\n",
    "\n",
    "    # Change final acceleration after stop to zero. Determined visually\n",
    "    print(\"Setting final acceleration at 0...\")\n",
    "    referenceTrajectory['refAccel_x'][4968:] = 0\n",
    "    \n",
    "    #%% Truth Gen Step 6 -  Integrate truth acceleration to get velocity and distance\n",
    "    referenceTrajectory['refVel_x'] = integrate.cumulative_trapezoid(y = referenceTrajectory['refAccel_x'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    \n",
    "    # Change final Velocity after stop to zero. Determined visually\n",
    "    print(\"Setting final velocity at 0...\")\n",
    "    referenceTrajectory['refVel_x'][4968:] = 0\n",
    "    \n",
    "    # Integrate velocity to get trajectory distance. \n",
    "    referenceTrajectory['refDist_x'] = integrate.cumulative_trapezoid(y = referenceTrajectory['refVel_x'],x = referenceTrajectory['Time'],initial = 0) \n",
    "\n",
    "    # Integrate EGI velocity to compare to double integrated acceleration\n",
    "    referenceTrajectory['refEGIDist_x'] = integrate.cumulative_trapezoid(y = referenceTrajectory['refEGIVel_x'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    \n",
    "    # Compute start motion time.\n",
    "    startMotionTime = referenceTrajectory['Time'][referenceTrajectory['refAccel_x']>0.001].iloc[0]\n",
    "    # Set the start motion time as 0. \n",
    "    referenceTrajectory['Time'] = referenceTrajectory['Time']-startMotionTime\n",
    "    \n",
    "    \n",
    "    #%% Save trajectory to Pickle File   \n",
    "    referenceTrajectory.to_pickle(\"./referenceTrajectory.pkl\")\n",
    "    \n",
    "    #%% Plots Acceleration and Velocity\n",
    "    if plotcheck == True:\n",
    "        Figure1 = PlotlyPlot()\n",
    "        Figure1.setTitle('EGI Acceleration, Velocity and Smoothed acceleration')\n",
    "        Figure1.setYaxisTitle('Acceleration (m/s/s)')\n",
    "        Figure1.setYaxis2Title('Velocity (m/s)')\n",
    "        Figure1.setXaxisTitle('GPS Time (s)')\n",
    "        Figure1.settwoAxisChoice([False, True])\n",
    "        Figure1.plotTwoAxis(referenceTrajectory[['Ax','Vx']], df_x= EGI_accel_vel_trim[['New Time']])\n",
    "        Figure1.addLine(referenceTrajectory[['refAccel_x']], df_x = referenceTrajectory[['Time']],secondary_y=False)\n",
    "        Figure1.show()\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f04590",
   "metadata": {},
   "source": [
    "#### generateTrackRPV()\n",
    "\n",
    "This function generates a reference position vector given a given reference trajector as created by generateReferenceTrajectory()\n",
    "\n",
    "\n",
    "Step 1: Set up interupter system.  \n",
    "The track reference is generated using interupter blades that are located approximately every 4.5 feet down the track. The system on the guidance sled records the time in which every interupter is passed and is then processed so that a downtrack distance of the sled is associated with every interupt.\n",
    "\n",
    "The interupters are located at fixed downtrack distances so the reference trajectory can be used to determine at what time each interupter would be passed.\n",
    "\n",
    "$$t=t_1+\\left(I-R(t_1)\\right) \\frac{t_2-t_1}{R(t_2)-R(t_1)}$$\n",
    "\n",
    "$R(t)$ = Reference Trajectory Down Track Distance at time $t$  \n",
    "$T$ = Reference Tracjectory Time  \n",
    "$I$ = Interupter Down Track Distance that falls between $R(t_2)$ and $R(t_1)$  \n",
    "$t$ = Time\n",
    "\n",
    "Step 2: Adding Error to Reference Position Vector  \n",
    "This code has the ability to add a variety of errors to the RPV. Most notably the ability to add random noise given.\n",
    "\n",
    "Noise is added using the following code (Zero mean, normal noise).  \n",
    "    ```noise = np.random.normal(0,sigmaRPV,len(trackRPV)) # Add random noise to RPV```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ebfefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTrackRPV(referenceTrajectory, sigmaRPV, tauRPV, biasRPV, Overwrite=True):\n",
    "    \n",
    "    # print(\"\\n Generating RPV...\")\n",
    "    trackRPV = pd.DataFrame()\n",
    "        \n",
    "    Interupter_delta = 4.5 * 0.3048 # ft converted to meters\n",
    "    TrackLength = 10000   # Meters  \n",
    "    \n",
    "    trackRPV['Interupters_DwnTrk_dist'] = np.arange(0, TrackLength, Interupter_delta)\n",
    "    trackRPV['Time'] = np.interp(trackRPV['Interupters_DwnTrk_dist'],referenceTrajectory['refDist_x'],referenceTrajectory['Time'])\n",
    "    \n",
    "    # Trim off the RPV so that there is no interupts past the time the sled stopped motion.\n",
    "    trackRPV = trackRPV[trackRPV['Interupters_DwnTrk_dist'] <= referenceTrajectory['refDist_x'].max()]\n",
    "    trackRPV = trackRPV.drop_duplicates(subset=['Time'])\n",
    "    trackRPV = trackRPV[:-1]\n",
    "    \n",
    "    ###########################################################################################################\n",
    "    #\n",
    "    # REMOVED ZERO VELOCITY CODE. \n",
    "    # Code can be found in original python code. Remove\n",
    "    # because adding areas of the trajectory where motion was\n",
    "    # zero seemed to hurt estimates. \n",
    "    #\n",
    "    ###########################################################################################################\n",
    "    \n",
    "    # Sort the values by time.\n",
    "    trackRPV = trackRPV.sort_values(by='Time').reset_index(drop=True)\n",
    "        \n",
    "    # Add error to Track RPV\n",
    "    if sigmaRPV != 0:\n",
    "        noise = np.random.normal(0,sigmaRPV,len(trackRPV)) # Add random noise to RPV\n",
    "        trackRPV['Interupters_DwnTrk_dist'] = trackRPV['Interupters_DwnTrk_dist'] + noise\n",
    "    \n",
    "    if tauRPV != 0: \n",
    "        trackRPV['Time'] = trackRPV['Time'] - tauRPV # Add time lag error\n",
    "        \n",
    "    if biasRPV != 0:\n",
    "        trackRPV['Interupters_DwnTrk_dist'] = trackRPV['Interupters_DwnTrk_dist'] + biasRPV # Add distance bias.\n",
    "\n",
    "    \n",
    "    ###########################################################################################################\n",
    "    #%% Save track RPV to pickle file\n",
    "    if Overwrite == True:\n",
    "        trackRPV.to_pickle(f\"./RPVs/trackRPV_sig{sigmaRPV}_tau{tauRPV}_bias{biasRPV}.pkl\")\n",
    "    else:\n",
    "       filepath = incrementFileName(f\"./VarianceRPVs/trackRPV_sig{sigmaRPV}_tau{tauRPV}_bias{biasRPV}.pkl\")\n",
    "       trackRPV.to_pickle(filepath)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53cb39",
   "metadata": {},
   "source": [
    "#### AccelSim()\n",
    "\n",
    "ACCEL SIM - Scripts used to generate simulated accelerometer output based on truth input. Uses smoothed acceleration reference trajectory data to simulate. The simulated acceleration, velocity and distance is saved to a PandasData frame.\n",
    "\n",
    "This method uses scipy's ```cumulative_trapezoid()``` to integrate acceleration to get velocity and distance. NOTE: This could potentially be improved with better methods.\n",
    "\n",
    "\n",
    "See [Accelerometer Class](#Accelerometer_Class) for details on the acceleromter error model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5790140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AccelSim(referenceTrajectory, N_model, changeDefaultCoeff, CoeffDict, g):\n",
    "    \n",
    "    #%% ACCEL SIM Step 1 - Simulate a Acceleromter with Bias using Accelerometer class\n",
    "    \"\"\"\n",
    "    ACCEL SIM - Scripts used to generate simulated accelerometer output based on truth input\n",
    "    \n",
    "    Using smoothed acceleration truth data to simulate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Intialize accelerometer class.\n",
    "    AccelOne = Accelerometer()\n",
    "    \n",
    "    # Change the default error model coeficients defined in the Accleromter class. \n",
    "    if changeDefaultCoeff == True:\n",
    "            AccelOne.AccelModelCoef.update(CoeffDict)\n",
    "    \n",
    "    # Create data frame to house data\n",
    "    sensorSim = pd.DataFrame()\n",
    "    sensorSim['Time'] = referenceTrajectory['Time']\n",
    "    \n",
    "    # Change to array for use in simulation.\n",
    "    A_i_true = referenceTrajectory['refAccel_x'].to_numpy()  \n",
    "    \n",
    "    # Simulate\n",
    "    A_x_sim = AccelOne.simulate(A_i_true, N_model[0], N_model[1])  \n",
    "    \n",
    "    # Store data in data frame. \n",
    "    sensorSim['SensorSim_Ax'] = A_x_sim\n",
    "    \n",
    "    # Accelerometer data is set to 0 prior to first motion. \n",
    "    sensorSim['SensorSim_Ax'][referenceTrajectory['refAccel_x'] == 0] = 0\n",
    "    \n",
    "    #%% Integrate Simulated accelerations to develop Velocity and Displacement.\n",
    "    sensorSim['SensorSim_Vx'] = integrate.cumulative_trapezoid(y = sensorSim['SensorSim_Ax'],x = sensorSim['Time'],initial = 0) \n",
    "    sensorSim['SensorSim_Dx'] = integrate.cumulative_trapezoid(y = sensorSim['SensorSim_Vx'],x = sensorSim['Time'],initial = 0) \n",
    "    \n",
    "    AccelObj = AccelOne\n",
    "    \n",
    "    return [sensorSim, AccelObj]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b8706",
   "metadata": {},
   "source": [
    "# RegressionAnalysis()\n",
    "\n",
    "This function performs a least squares regression analysis on the simulated data to determine the error coefficients of the accelerometer.\n",
    "\n",
    "The code you provided is performing a few mathematical operations. Here are the corresponding LaTeX equations:\n",
    "\n",
    "1. Interpolation of Sensor Sim to Track:\n",
    "   The code is interpolating the `SensorSim_Dx` values from `sensorSim` at the times specified in `trackRPV['Time']`. This can be represented by the following equation:\n",
    "\n",
    "   $$SensorDwnTrkDist(t) = \\text{interp}(t, sensorSim.Time, sensorSim.SensorSim\\_Dx)$$\n",
    "\n",
    "   where `interp` is the interpolation function, `t` is the time from `trackRPV['Time']`, and `sensorSim.Time` and `sensorSim.SensorSim_Dx` are the time and SensorSim_Dx columns from the sensorSim dataframe respectively.\n",
    "\n",
    "2. Distance Error Calculation:\n",
    "   The code is calculating the distance error as the difference between `Interupters_DwnTrk_dist` and `SensorDwnTrkDist`. This can be represented by the following equation:\n",
    "\n",
    "   $$DistErr\\_x = Interupters\\_DwnTrk\\_dist - SensorDwnTrkDist$$\n",
    "\n",
    "3. Velocity Error Calculation:\n",
    "   The code is calculating the velocity error as the difference in distance error over the difference in time. This can be represented by the following equation:\n",
    "\n",
    "   $$Ve\\_x = \\frac{\\Delta DistErr\\_x}{\\Delta t}$$\n",
    "\n",
    "   where $\\Delta DistErr\\_x$ is the difference in distance error and $\\Delta t$ is the difference in time.\n",
    "\n",
    "4. Time Calculation for Velocity Error:\n",
    "   The code is calculating the time associated with each velocity error as the average of two consecutive times. This can be represented by the following equation:\n",
    "\n",
    "   $$Ve\\_t = \\frac{t_{i} + t_{i+1}}{2}$$\n",
    "\n",
    "   where $t_{i}$ and $t_{i+1}$ are two consecutive times from `Dist_Error['Time']`.\n",
    "   \n",
    "   \n",
    "  \n",
    "\n",
    "5. If `sigmaRPV` is zero or `WLSoption` is false, then the size of matrix `W` is set to an identity matrix of the same size as matrix `A`. The covariance matrix `covariance_A` is set to 'Covariance Not Computed'.\n",
    "6. Otherwise, the code computes the velocity error uncertainty `vel_sig` using the difference between consecutive elements of array `Ve_t`, which is stored in variable `delta_t`. The formula used for computing `vel_sig` is: \n",
    "\n",
    "$$ vel\\_sig = \\sqrt{2} \\times \\frac{\\sigma_{RPV}}{\\Delta t} $$\n",
    "\n",
    "where $\\sigma_{RPV}$ is a constant and $\\Delta t$ is an array of time differences.\n",
    "\n",
    "7. The code then computes the velocity error variance `vel_var` by squaring each element of array `vel_sig`.\n",
    "8. The code then computes a weighted least squares weighting matrix `w` using the formula:\n",
    "\n",
    "$$ w = diag(vel\\_var,0) - diag(0.499 \\times vel\\_var[:-1],-1) - diag(0.499 \\times vel\\_var[:-1],1) $$\n",
    "\n",
    "where `diag()` creates a diagonal matrix with the given input array as its diagonal elements.\n",
    "9. The code then computes the inverse of matrix `w` and stores it in variable `W`.\n",
    "10. The code checks if there are any negative values in the diagonal of matrix `W`. If there are any negative values, it prints a message indicating the total number of negative values detected.\n",
    "11. Finally, the code saves matrix `w` as a CSV file named 'wMatrix' and computes the Cholesky decomposition of matrix `w`.\n",
    "\n",
    "\n",
    "The difference between the reference position vector and the simulated accelerometer can be taken by first integrating the accelerometer values twice to get the simulated reported down track distance of the accelerometer. The  track reference position vector and simulated accelerometer distance is differences to get distance error. Then the distance error is differentiated with respect to time to get Velocity error. \n",
    "\n",
    "$$\n",
    "D_e = D_{rpv} - D_{sim}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_e = \\frac{dD_e}{dt}\n",
    "$$\n",
    "\n",
    "Integrating the initial accelerometer error equation provides an equation relating the velocity error to the error model coefficients.   \n",
    "\n",
    "$$\n",
    "\\overline{V_{e}}=V_0 + K_{0}t+K_{1} V_{X}+K_{2} \\int A_{X}^{2}dt+K_{3}\\int A_{X}^{3}dt+K_{4}\\int A_{X}^{4}dt+K_{5}\\int A_{X}^{5}dt\n",
    "$$\n",
    "\n",
    "The true velocity error is substituted with the error computer from the difference between the track RPV and simulated accelerometer. The equations can be rewritten as:\n",
    "\n",
    "$$\n",
    "V_e = Ax\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & t & V_x & \\int A_{X}^{2}dt & \\int A_{X}^{3}dt & \\int A_{X}^{4}dt & \\int A_{X}^{5}dt\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix} K_0 & K_1 & K_2 & K_3 & K_4 & K_5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The coordinate functions (columns of A (reference IEEE)?) Are typically computed using the unit under test reported acceleration but since the true trajectory is know the coordinate functions are computed using the reference trajectory acceleration. For purposes of this study a simple cumulative trapezoid method was used to compute all integrations. \n",
    "\n",
    "To compute and estimate for the error model coefficients a least squares estimation is applied for all the values across the trajectory of the sled launch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25b4ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegressionAnalysis(referenceTrajectory, trackRPV, AccelObj, sensorSim, N_model, g,sigmaRPV, startStopTime = [0, -1], saveToPickel = False, WLSoption = True, LeastSquaresMethod = 'LongHand',computeCovariance = True):\n",
    "\n",
    "    \"\"\"\n",
    "    Error - Scripts used to compare accelerometer simulation versus track truth\n",
    "    \"\"\"\n",
    "    Dist_Error = pd.DataFrame()\n",
    "    Dist_Error['Time'] = trackRPV['Time']\n",
    "    \n",
    "    ##############################################################################################################\n",
    "    # Step 1: Interpolate Sensor Sim to Track \n",
    "    ##############################################################################################################\n",
    "\n",
    "    trackRPV['SensorDwnTrkDist'] = np.interp(trackRPV['Time'],sensorSim['Time'],sensorSim['SensorSim_Dx'])    \n",
    "    Dist_Error['DistErr_x'] = trackRPV['Interupters_DwnTrk_dist'] - trackRPV['SensorDwnTrkDist']\n",
    "    \n",
    "    ##############################################################################################################\n",
    "    # Step 2: Compute Velocity Error\n",
    "    ##############################################################################################################\n",
    "    # Compute Velocity Error\n",
    "    Ve_x = (np.diff(Dist_Error['DistErr_x'])/np.diff(Dist_Error['Time']))\n",
    "    Ve_t = (Dist_Error['Time'].head(-1) + np.diff(Dist_Error['Time'])/2).to_numpy()\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Trim Ve_t and Ve_x to start and stop time.\n",
    "    # first if startStopTime[1] == -1 then set stop time to end of data.\n",
    "    if startStopTime[1] == -1:\n",
    "        startStopTime = (startStopTime[0], Ve_t[-1])\n",
    "\n",
    "    Ve_x = Ve_x[(Ve_t > startStopTime[0]) & (Ve_t <= startStopTime[1])]\n",
    "    Ve_t = Ve_t[(Ve_t > startStopTime[0]) & (Ve_t <= startStopTime[1])]\n",
    "    \n",
    "    # Remove first two indicies as it causes problems with covariance matrix.\n",
    "    idxStart = 10\n",
    "    idxEnd = -10\n",
    "\n",
    "    Ve_x = Ve_x[idxStart:idxEnd]\n",
    "    Ve_t = Ve_t[idxStart:idxEnd]\n",
    "\n",
    "    Error = pd.DataFrame()\n",
    "    \n",
    "    Error['Time'] = Ve_t\n",
    "    Error['SensorSim_Ax'] = np.interp(Ve_t,sensorSim['Time'],sensorSim['SensorSim_Ax']) \n",
    "    Error['SensorSim_Vx'] = np.interp(Ve_t,sensorSim['Time'],sensorSim['SensorSim_Vx'])\n",
    "    Error['SensorSim_Dx'] = np.interp(Ve_t,sensorSim['Time'],sensorSim['SensorSim_Dx'])\n",
    "    Error['DistErr_x'] = np.interp(Ve_t,Dist_Error['Time'],Dist_Error['DistErr_x']) \n",
    "    Error['VelErr_x'] = Ve_x\n",
    "     \n",
    "    #%% - Regression Analysis\n",
    "    \"\"\"\n",
    "    Regression Analysis - Scripts used to compute error model\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ##############################################################################################################\n",
    "    # Step X: Build A matrix based on accelerometer error model\n",
    "    ##############################################################################################################\n",
    "    \n",
    "    # # Compute coordinate functions\n",
    "    referenceTrajectory['Ax^2 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**2\n",
    "    referenceTrajectory['Ax^3 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**3\n",
    "    referenceTrajectory['Ax^4 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**4\n",
    "    referenceTrajectory['Ax^5 (g)'] = (referenceTrajectory[['refAccel_x']]/g)**5\n",
    "    \n",
    "    referenceTrajectory['intAx^2 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^2 (g)'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    referenceTrajectory['intAx^3 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^3 (g)'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    referenceTrajectory['intAx^4 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^4 (g)'],x = referenceTrajectory['Time'],initial = 0)\n",
    "    referenceTrajectory['intAx^5 (g)'] = -integrate.cumulative_trapezoid(y = referenceTrajectory['Ax^5 (g)'],x = referenceTrajectory['Time'],initial = 0) \n",
    "    \n",
    "    \n",
    "    Vx = np.interp(Ve_t, referenceTrajectory['Time'],referenceTrajectory['refVel_x'])\n",
    "    intAx_2 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^2 (g)']) \n",
    "    intAx_3 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^3 (g)']) \n",
    "    intAx_4 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^4 (g)']) \n",
    "    intAx_5 = np.interp(Ve_t,referenceTrajectory['Time'],referenceTrajectory['intAx^5 (g)'])\n",
    "    \n",
    "    coordinateFunctionDF = pd.DataFrame()\n",
    "    coordinateFunctionDF['Time'] = Ve_t\n",
    "     \n",
    "    coeff_dict = {'Est_V_0': 0, 'Est_K_1': 0, 'Est_K_0': 0, 'Est_K_2': 0, 'Est_K_3': 0, 'Est_K_4': 0, 'Est_K_5': 0}\n",
    "\n",
    "    # Create Complete A Matrix\n",
    "    complete_A = np.array([np.ones(len(Ve_t))/g, -Vx/g, -Ve_t, intAx_2, intAx_3, intAx_4, intAx_5])*g\n",
    "    complete_A = complete_A.T\n",
    "    \n",
    "    complete_A_DF = pd.DataFrame(np.fliplr(complete_A), columns=['IntAx_5', 'IntAx_4', 'IntAx_3', 'IntAx_2', 'Ve_t', 'Vx', 'Ones'])\n",
    "    \n",
    "    trimmed_A_filt = np.zeros(complete_A.shape[1], dtype = bool)\n",
    "    trimmed_A_filt[0] = 1\n",
    "    \n",
    "    trimmed_A_filt[N_model[0]+1:N_model[1]+1] = 1\n",
    "\n",
    "    trimmed_A = complete_A[:,trimmed_A_filt]\n",
    "\n",
    "    # Remove the first row of trimmed_A matrix\n",
    "    A = trimmed_A[1:,:]    \n",
    "    At = np.transpose(A)\n",
    "    \n",
    "    '''\n",
    "    COMPUTE COVARIANCE\n",
    "    '''\n",
    "    \n",
    "    ##############################################################################################################\n",
    "    # Step X: Build the Weighting matrix and compute covariance matrix.\n",
    "    ##############################################################################################################\n",
    "    \n",
    "    \n",
    "    #%% \n",
    "    # Linear Regression\n",
    "    coeff_list = tuple(None for _ in range(trimmed_A.shape[1]))\n",
    "\n",
    "    if sigmaRPV == 0 or WLSoption == False: \n",
    "        # Remove first row of A to make consistent size with W\n",
    "        size = A.shape[0]\n",
    "        W = np.identity(size)\n",
    "        covariance_A = 'Covariance Not Computed'\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        ################################################################################################\n",
    "        # Develop weighted matrix\n",
    "        ################################################################################################\n",
    "        # Compute Velocity Error Uncertainty\n",
    "        delta_t = np.diff(Ve_t)\n",
    "        vel_sig = np.sqrt(2)*sigmaRPV/delta_t\n",
    "        \n",
    "        # Compute Velocity Error Variance\n",
    "        vel_var = np.square(vel_sig)\n",
    "        \n",
    "        # Computed Weighted Least Squares Weighting Matrix\n",
    "        w = np.diag(vel_var,0) - np.diag((.5*vel_var[:-1]),-1) - np.diag((.5*vel_var[:-1]),1)  \n",
    "        W = np.linalg.inv(w)\n",
    "        # W = np.diag(1/vel_sig) \n",
    "\n",
    "        ################################################################################################     \n",
    "        ## TODO\n",
    "        ## ADD Noise to velocity then add weighted velocity. \n",
    "        ## Try covariance on just individual coefficients. \n",
    "        ## Figure out if covariances are always off by certain amount.\n",
    "        ################################################################################################\n",
    "\n",
    "        # pdb.set_trace()    \n",
    "            \n",
    "        negative_values = np.sum(np.diag(W) < 0)\n",
    "        if negative_values > 0:\n",
    "            print(f\"Negative value detected in the Weighting matrix. Total negative values: {negative_values}\")\n",
    "            \n",
    "        wPndDF = pd.DataFrame(w)\n",
    "        wPndDF.to_csv('wMatrix')\n",
    "        \n",
    "        # Compute the cholesky of w (banded). Then do the inverse. \n",
    "     \n",
    "        ################################################################################################\n",
    "        # Compute Covariance Matrix\n",
    "        ################################################################################################    \n",
    "        # Compute Covariance  \n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        if computeCovariance == True:\n",
    "            covariance_A = np.linalg.inv(At.dot(W).dot(A))\n",
    "        else:\n",
    "            covariance_A = 'Covariance Not Computed'\n",
    "    \n",
    "    \n",
    "    ##############################################################################################################\n",
    "    # Step X: Perform Least Squares\n",
    "    ##############################################################################################################\n",
    "    \n",
    "    \n",
    "    if LeastSquaresMethod == 'Numpy':\n",
    "        AW = np.transpose(trimmed_A).dot(W)\n",
    "        Ve_xW = W.dot(Ve_x)\n",
    "        coeff_list = np.linalg.lstsq(np.transpose(AW), Ve_xW, rcond=None)[0] # This has just been used for debugging to check if \"Long\" least squares leads to same results.\n",
    "    elif LeastSquaresMethod == 'SciKit':\n",
    "        testSKlearn = LinearRegression()\n",
    "        testSKlearn.fit(trimmed_A, Ve_x, sample_weight=(np.diag(W)))\n",
    "        coeff_list = testSKlearn.coef_\n",
    "        coeff_list[0] = testSKlearn.intercept_\n",
    "    elif LeastSquaresMethod == 'LongHand':\n",
    "        coeff_list = np.linalg.inv(At.dot(W).dot(A)).dot(At).dot(W).dot(Ve_x[1:])\n",
    "        \n",
    "    else: \n",
    "        print(\"Did not select an applicable Least Squares Method\")\n",
    "\n",
    "    print_List = np.array(list(coeff_dict.keys()))\n",
    "    \n",
    "    n = 0\n",
    "    for coef in print_List[trimmed_A_filt]:\n",
    "        coeff_dict[coef] = coeff_list[n]\n",
    "        n += 1\n",
    "    \n",
    "    #%% Save results to DataFrame\n",
    "    coefficientDF = pd.DataFrame()\n",
    "    \n",
    "    coefficientDF = pd.concat((coefficientDF, pd.DataFrame.from_dict(AccelObj.AccelModelCoef, orient = 'index', columns= ['Accel Model'])))\n",
    "    coefficientDF.loc['V_0'] = 0\n",
    "    \n",
    "    # Build Estimated Coefficient DF\n",
    "    estimatedCoefficients = pd.DataFrame.from_dict(coeff_dict, orient = 'index', columns= ['Estimated Coefficients'])\n",
    "    renameDict = {}\n",
    "    for coeff in print_List:\n",
    "        renameDict[coeff] = coeff[4:]\n",
    "    estimatedCoefficients = estimatedCoefficients.rename(index = renameDict) \n",
    "    estimatedCoefficients.replace(0, np.nan, inplace=True)\n",
    "    \n",
    "    \n",
    "    coefficientDF = pd.merge(coefficientDF,estimatedCoefficients,left_index=True, right_index=True)\n",
    "    \n",
    "    coefficientDF['Coefficient Estimate Error'] = coefficientDF['Accel Model'] - coefficientDF['Estimated Coefficients']\n",
    "    \n",
    "                \n",
    "    #%% Compute Velocity Error Residuals\n",
    "    V_error_model_terms = [coeff_dict['Est_V_0'], \n",
    "                           coeff_dict['Est_K_1']*Vx,  \n",
    "                           coeff_dict['Est_K_0']*Ve_t, \n",
    "                           coeff_dict['Est_K_2']*intAx_2, \n",
    "                           coeff_dict['Est_K_3']*intAx_3,  \n",
    "                           coeff_dict['Est_K_4']*intAx_4,  \n",
    "                           coeff_dict['Est_K_5']*intAx_5]\n",
    "    Error['V_error_model'] = sum(V_error_model_terms)*g \n",
    "    Error['Ve_x_Resid'] = Error['VelErr_x'] - Error['V_error_model']     \n",
    "\n",
    "    #%% Save off results:\n",
    "    if saveToPickel == True:\n",
    "        Error.to_pickle(f\"./ErrorDF_{N_model[0]}-{N_model[1]}.pkl\")\n",
    "        coefficientDF.to_pickle(f\"./coefficientDF_{N_model[0]}-{N_model[1]}.pkl\")\n",
    "        \n",
    "    return [coefficientDF, Error, covariance_A, A, Ve_x, W, LeastSquaresMethod]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1fa6b0",
   "metadata": {},
   "source": [
    "### Set initial coefficients and parameters for script\n",
    "\n",
    "Set the initial configuration parameters and logic that drives how results are computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a20b5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#%% Initial Configuration Parameters\n",
    "############################################################\n",
    "\n",
    "# Set value for g\n",
    "\n",
    "g = 9.791807  \n",
    "\n",
    "\n",
    "# Set parameters for error added to Reference Position Vector\n",
    "\n",
    "sigmaRPV = 0.001        # Standard deviation of Random noise centered at zero added to downtrack distance (meters)\n",
    "tauRPV =  0            # Time Lag Error (seconds)\n",
    "biasRPV = 0            # Bias error in RPV (meters) \n",
    "\n",
    "# Set number of Monte Carlo Runs\n",
    "\n",
    "MCnum = 1\n",
    "\n",
    "# Set custom coefficients for Accelerometer error model. Updates accelerometer coefficient error model \n",
    "# dictionary if ChangeDefaultCoeff is set to True.\n",
    "\n",
    "CoeffDict = {'K_0': 5E-8}\n",
    "\n",
    "# Used to determine how many coefficients to calculate.\n",
    "\n",
    "N_model_start = 2     #  0 =  K_1 (Scale Factor), 1 = K_0 (Bias), 2 = K_2, etc. \n",
    "N_model_end = 5      #  0 = K_1 (Scale Factor), 1 = K_0 (Bias), 2 = K_2, etc.\n",
    "\n",
    "\n",
    "# Clean up Model indicies and define Error Coefficient Names\n",
    "N_model = [0,0]\n",
    "# Fix indexing numbers\n",
    "N_model[0] = N_model_start  ### REVIEW THIS\n",
    "N_model[1]= N_model_end + 1\n",
    "\n",
    "# Definition of corresponding coefficient names that will be computed based on above pararmeters\n",
    "ModelList = ['K_1', 'K_0', 'K_2', 'K_3','K_4','K_5']\n",
    "\n",
    "\n",
    "############################################################\n",
    "#%% Initial Configuration Logic\n",
    "############################################################\n",
    "\n",
    "# If set to True, accelerometer model error will be updated with CoeffDict values set in intial parameters.\n",
    "\n",
    "changeDefaultCoeff = True\n",
    "\n",
    "\n",
    "# Generate New Trajectory. If set to True new Trajectory will be created and saved to .pkl file from EGI data.\n",
    "\n",
    "generateNewTrajectory = False\n",
    "\n",
    "\n",
    "# Generate New RPV. If set to True a new RPV will be generated and saved to .pkl file. If set to False, code will \n",
    "# to make sure and RPV with the parameters set in the intial configuration is available. If not availble a new RPV \n",
    "# be generated. \n",
    "generateNewRPV = True\n",
    "\n",
    "\n",
    "# LeastSquaresMethod sets the method used for Least Squares Regression analaysis. Default is set to 'LongHand'\n",
    "#  - 'LongHand':  Computes the least squares using numpy matrix multiplication. This is the only method that works for \n",
    "#                 Weighted Least Squares with correleated off diagonal values in the weighting matrix.\n",
    "#  - 'Numpy':     Uses the least squares function from the numpy.linalg library. This method should not be used if using any sort of weighted least squares method.\n",
    "#  - 'SciKit':    Computes the least squares regression using the SciKit library. This does not use any correlated off diagonal values. \n",
    "\n",
    "LeastSquaresMethod = 'LongHand'\n",
    "\n",
    "# If set to True the least squares regression method for determining error coefficients will use a \n",
    "# Weighted Least Squares Method.\n",
    "\n",
    "WLS = True\n",
    "\n",
    "# If set to true the model will perform regression analysis for each term indiviually as well as the full model as defined above or look at each individual coefficient.\n",
    "\n",
    "individualCoeffAnalysis = True\n",
    "\n",
    "# Set start and stop time (seconds) to perform regression analysis on.\n",
    "startStopTime = (0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998c2f4",
   "metadata": {},
   "source": [
    "## Run Monte Carlo Simulations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46391d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed MC runs\n"
     ]
    }
   ],
   "source": [
    "# Intialize Data Storage variables\n",
    "coefficientEstimates = [];\n",
    "\n",
    "# Run Monte Carlo Sims\n",
    "for mc in range(MCnum):\n",
    "\n",
    "    print(f'Running MC {mc}...', end=\"\\r\")  \n",
    "    \n",
    "    ########################t######################################################################################\n",
    "    #%% Generate or import trajectory\n",
    "    if generateNewTrajectory == True:      \n",
    "        generateReferenceTrajectory()\n",
    "\n",
    "    # Import Reference Trajectory\n",
    "    referenceTrajectory = pd.read_pickle(\"./referenceTrajectory.pkl\")\n",
    "\n",
    "\n",
    "    ##############################################################################################################\n",
    "    #%% Generate track reference position vectory\n",
    "\n",
    "    # If generateNewRPV is set to False, check if an RPV exists with the specified parameters. If it does not\n",
    "    # then set generateNewRPV to True so tha generateNewRPV runs anyways.\n",
    "    if generateNewRPV == False:   \n",
    "        generateNewRPV = not os.path.isfile(f\"./RPVs/trackRPV_sig{sigmaRPV}_tau{tauRPV}_bias{biasRPV}.pkl\")\n",
    "\n",
    "    if generateNewRPV == True:    \n",
    "        generateTrackRPV(referenceTrajectory, sigmaRPV, tauRPV, biasRPV)\n",
    "\n",
    "    # Import trackRPV pickle file that matches configuration parameters\n",
    "    trackRPV = pd.read_pickle(f\"./RPVs/trackRPV_sig{sigmaRPV}_tau{tauRPV}_bias{biasRPV}.pkl\")\n",
    "\n",
    "\n",
    "    ##############################################################################################################\n",
    "    #%% Generate Simulated Accelerometer for full model\n",
    "    sensorSim, AccelObj = AccelSim(referenceTrajectory, N_model, changeDefaultCoeff, CoeffDict, g)\n",
    "    \n",
    "    \n",
    "    #%% Perform Regression Analysis for full model\n",
    "    if mc == 0:\n",
    "        coefficientDF, Error, cov_A, A, Ve_x, W, LeastSquaresMethod = RegressionAnalysis(referenceTrajectory, trackRPV, AccelObj, sensorSim, N_model, g, sigmaRPV, startStopTime, WLSoption = WLS, computeCovariance = True)\n",
    "        coefficientCovariance = cov_A\n",
    "    else:\n",
    "        coefficientDF, Error, cov_A, A, Ve_x, W, LeastSquaresMethod = RegressionAnalysis(referenceTrajectory, trackRPV, AccelObj, sensorSim, N_model, g, sigmaRPV, startStopTime, WLSoption = WLS, computeCovariance = False)\n",
    "  \n",
    "    coefficientEstimates.append(coefficientDF)\n",
    "    \n",
    "print('Completed MC runs')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce8d29",
   "metadata": {},
   "source": [
    "# Monte Carlo Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3890e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "CoefficientEstimateErrors = pd.DataFrame()\n",
    "\n",
    "K_1_Errors = []\n",
    "K_0_Errors = []\n",
    "K_2_Errors = []\n",
    "K_3_Errors = []\n",
    "K_4_Errors = []\n",
    "K_5_Errors = []\n",
    "\n",
    "for n in range(len(coefficientEstimates)):\n",
    "    K_1_Errors.append(coefficientEstimates[n]['Coefficient Estimate Error']['K_1'])\n",
    "    K_0_Errors.append(coefficientEstimates[n]['Coefficient Estimate Error']['K_0'])\n",
    "    K_2_Errors.append(coefficientEstimates[n]['Coefficient Estimate Error']['K_2'])\n",
    "    K_3_Errors.append(coefficientEstimates[n]['Coefficient Estimate Error']['K_3'])\n",
    "    K_4_Errors.append(coefficientEstimates[n]['Coefficient Estimate Error']['K_4'])\n",
    "    K_5_Errors.append(coefficientEstimates[n]['Coefficient Estimate Error']['K_5'])\n",
    "    \n",
    "CoefficientEstimateErrors['K_1_Errors'] = K_1_Errors\n",
    "CoefficientEstimateErrors['K_0_Errors'] = K_0_Errors\n",
    "CoefficientEstimateErrors['K_2_Errors'] = K_2_Errors\n",
    "CoefficientEstimateErrors['K_3_Errors'] = K_3_Errors\n",
    "CoefficientEstimateErrors['K_4_Errors'] = K_4_Errors\n",
    "CoefficientEstimateErrors['K_5_Errors'] = K_5_Errors\n",
    "\n",
    "Variances = np.diag(coefficientCovariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "888103a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_2 Variance (real vs estimated): 2.726650201527623e-12   9.093680990971302e-10\n",
      "K_3 Variance (real vs estimated): 1.1580929606381783e-13   4.237029726734616e-11\n",
      "K_4 Variance (real vs estimated): 2.2181683818822318e-15   5.453566831763081e-13\n",
      "K_5 Variance (real vs estimated): 3.864136235658828e-17   1.0768082779895156e-14\n"
     ]
    }
   ],
   "source": [
    "# If length of Coefficient Estimate Errors dataframe is 3 or more\n",
    "if len(CoefficientEstimateErrors) >= 3:\n",
    "    # print('K_1 Variance (real vs estimated):', CoefficientEstimateErrors['K_1_Errors'].std()**2, ' ', Variances[1])\n",
    "    # print('K_0 Variance (real vs estimated):', CoefficientEstimateErrors['K_0_Errors'].std()**2, ' ', Variances[2])\n",
    "\n",
    "    print('K_2 Variance (real vs estimated):', CoefficientEstimateErrors['K_2_Errors'].std()**2, ' ', Variances[1])\n",
    "    print('K_3 Variance (real vs estimated):', CoefficientEstimateErrors['K_3_Errors'].std()**2, ' ', Variances[2])\n",
    "    print('K_4 Variance (real vs estimated):', CoefficientEstimateErrors['K_4_Errors'].std()**2, ' ', Variances[3])\n",
    "    print('K_5 Variance (real vs estimated):', CoefficientEstimateErrors['K_5_Errors'].std()**2, ' ', Variances[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.87516465e-06 2.97183095e-10 6.62082667e-12 2.50916053e-11\n",
      " 2.88427774e-12 1.30930020e-14 4.40858238e-16]\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(Variances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7ba2dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_1 Mean: nan\n",
      "K_0 Mean: nan\n",
      "K_2 Mean: 2.5409974483267055e-07\n",
      "K_3 Mean: 8.047165105291427e-08\n",
      "K_4 Mean: -8.461925036853243e-09\n",
      "K_5 Mean: -1.3780192608680843e-09\n"
     ]
    }
   ],
   "source": [
    "print('K_1 Mean:', CoefficientEstimateErrors['K_1_Errors'].mean())\n",
    "print('K_0 Mean:', CoefficientEstimateErrors['K_0_Errors'].mean())\n",
    "print('K_2 Mean:', CoefficientEstimateErrors['K_2_Errors'].mean())\n",
    "print('K_3 Mean:', CoefficientEstimateErrors['K_3_Errors'].mean())\n",
    "print('K_4 Mean:', CoefficientEstimateErrors['K_4_Errors'].mean())\n",
    "print('K_5 Mean:', CoefficientEstimateErrors['K_5_Errors'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "138943d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Accel Model  Estimated Coefficients  Coefficient Estimate Error\n",
      "K_1  5.000000e-06                     NaN                         NaN\n",
      "K_0  5.000000e-08                     NaN                         NaN\n",
      "K_2  6.114000e-05            5.929199e-05                1.848014e-06\n",
      "K_3  2.000000e-08           -4.262237e-07                4.462237e-07\n",
      "K_4  6.000000e-09            6.212808e-08               -5.612808e-08\n",
      "K_5  2.300000e-09            1.022777e-08               -7.927774e-09\n",
      "V_0  0.000000e+00           -1.046255e-03                1.046255e-03\n"
     ]
    }
   ],
   "source": [
    "print(coefficientDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "      Accel Model  Estimated Coefficients  Coefficient Estimate Error\n",
    "K_1  5.000000e-06                     NaN                         NaN\n",
    "K_0  5.000000e-09                     NaN                         NaN\n",
    "K_2  6.114000e-05            6.114068e-05               -6.785534e-10\n",
    "K_3  2.000000e-08            1.966873e-08                3.312732e-10\n",
    "K_4  6.000000e-09            5.995298e-09                4.702449e-12\n",
    "K_5  2.300000e-09            2.304385e-09               -4.385152e-12\n",
    "V_0  0.000000e+00           -4.434971e-07                4.434971e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37984cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8074f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f60b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
